file,book,conference,DO,TY,TI,T2,SP,EP,PY,JO,IS,SN,VO,VL,JA,Y1,PB,UR,AU,KW,AB
TVCG-2020-1.ris,TVCG,prologue,10.1109/TVCG.2019.2935603,JOUR,IEEE Transactions on Visualization and Computer Graphics,IEEE Transactions on Visualization and Computer Graphics,i,i,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2935603,[],[],"Presents a listing of the editorial board, board of governors, current staff, committee members, and/or society editors for this issue of the publication."
TVCG-2020-1.ris,TVCG,prologue,10.1109/TVCG.2019.2935654,JOUR,Contents,IEEE Transactions on Visualization and Computer Graphics,ii,ix,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2935654,[],[],Presents the table of contents for this issue of this publication.
TVCG-2020-1.ris,TVCG,prologue,10.1109/TVCG.2019.2935643,JOUR,Message from the Editor-in-Chief,IEEE Transactions on Visualization and Computer Graphics,x,x,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2935643,"['Mueller, K.']",[],Presents the introductory editorial for this issue of the publication.
TVCG-2020-1.ris,TVCG,prologue,10.1109/TVCG.2019.2935644,JOUR,Preface,IEEE Transactions on Visualization and Computer Graphics,xi,xvi,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2935644,"['Chang, R.', 'Fujishiro, I.', 'Isenberg, P.', 'Keim, D.', 'Lindstrom, P.', 'Maciejewski, R.', 'Meyer, M.', 'Weber, G. H.', 'Weiskopf, D.', 'Wood, J.']",[],"This January 2020 issue of the IEEE Transactions on Visualization and Computer Graphics (TVCG) contains the proceedings of IEEE VIS 2019, held during 20-25 October 2019 at the Vancouver Convention Center in Vancouver, BC, Canada. With IEEE VIS 2019, the conference series is in its 30 th year. IEEE VIS consists of three conferences, held concurrently: the IEEE Visual Analytics Science and Technology Conference (VAST), the IEEE Information Visualization Conference (InfoVis), and the IEEE Scientific Visualization Conference (SciVis). These three conferences are the premier venues for the visualization community to exchange the latest ideas and developments, attracting researchers and practitioners alike."
TVCG-2020-1.ris,TVCG,prologue,10.1109/TVCG.2019.2935653,JOUR,IEEE Visualization and Graphics Technical Committee (VGTC): http://vgtc.org/,IEEE Transactions on Visualization and Computer Graphics,xvii,xvii,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2935653,[],[],Provides a listing of current committee members and society officers.
TVCG-2020-1.ris,TVCG,prologue,10.1109/TVCG.2019.2935609,JOUR,VIS Conference Committee,IEEE Transactions on Visualization and Computer Graphics,xviii,xix,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2935609,[],[],Provides a listing of current committee members and society officers.
TVCG-2020-1.ris,TVCG,prologue,10.1109/TVCG.2019.2935657,JOUR,VAST International Program Committee,IEEE Transactions on Visualization and Computer Graphics,xx,xx,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2935657,[],[],Provides a listing of current committee members and society officers.
TVCG-2020-1.ris,TVCG,prologue,10.1109/TVCG.2019.2935658,JOUR,InfoVis International Program Committee,IEEE Transactions on Visualization and Computer Graphics,xxi,xxi,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2935658,[],[],Provides a listing of current committee members and society officers.
TVCG-2020-1.ris,TVCG,prologue,10.1109/TVCG.2019.2935659,JOUR,SciVis International Program Committee,IEEE Transactions on Visualization and Computer Graphics,xxii,xxii,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2935659,[],[],Provides a listing of current committee members and society officers.
TVCG-2020-1.ris,TVCG,prologue,10.1109/TVCG.2019.2935637,JOUR,Committees,IEEE Transactions on Visualization and Computer Graphics,xxiii,xxiii,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2935637,[],[],"Presents a listing of the editorial board, board of governors, current staff, committee members, and/or society editors for this issue of the publication."
TVCG-2020-1.ris,TVCG,prologue,10.1109/TVCG.2019.2935660,JOUR,VAST Paper Reviewers,IEEE Transactions on Visualization and Computer Graphics,xxiv,xxv,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2935660,[],[],The conference offers a note of thanks and lists its reviewers.
TVCG-2020-1.ris,TVCG,prologue,10.1109/TVCG.2019.2935639,JOUR,InfoVis Paper Reviewers,IEEE Transactions on Visualization and Computer Graphics,xxvi,xxvii,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2935639,[],[],The conference offers a note of thanks and lists its reviewers.
TVCG-2020-1.ris,TVCG,prologue,10.1109/TVCG.2019.2935640,JOUR,SciVis Paper Reviewers,IEEE Transactions on Visualization and Computer Graphics,xxviii,xxviii,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2935640,[],[],The conference offers a note of thanks and lists its reviewers.
TVCG-2020-1.ris,TVCG,prologue,10.1109/TVCG.2019.2935641,JOUR,The 2019 Visualization Career Award,IEEE Transactions on Visualization and Computer Graphics,xxix,xxix,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2935641,"['Ertl, T.']",[],"The 2019 Visualization Career Award goes to Thomas Ertl, Thomas Ertl is a Professor of Computer Science at the University of Stuttgart where he founded the Institute for Visualization and Interactive Systems (VIS) and the Visualization Research Center (VISUS). He received a MSc in Computer Science from the University of Colorado at Boulder and a PhD in Theoretical Astrophysics from the University of Tuebingen. After a few years as postdoc and cofounder of a Tuebingen based IT company, he moved to the University of Erlangen as a Professor of Computer Graphics and Visualization. He served the University of Stuttgart in various administrative roles including Dean of Computer Science and Electrical Engineering and Vice President for Research and Advanced Graduate Education. Currently, he is the Spokesperson of the Cluster of Excellence Data-Integrated Simulation Science and Director of the Stuttgart Center for Simulation Science."
TVCG-2020-1.ris,TVCG,prologue,10.1109/TVCG.2019.2935661,JOUR,The 2019 Visualization Technical Achievement Award,IEEE Transactions on Visualization and Computer Graphics,xxx,xxx,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2935661,"['Groller, E.']",[],"The 2019 Visualization Technical Achievement Award goes to Eduard Groller, Eduard Groller is Professor at the Institute of Visual Computing & Human-Centered Technology (VC&HCT), Vienna University of Technology. In 1993 he received his PhD from the same university. His research interests include computer graphics, visualization, and visual computing."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934668,JOUR,FlowSense: A Natural Language Interface for Visual Data Exploration within a Dataflow System,IEEE Transactions on Visualization and Computer Graphics,1,11,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934668,"['Yu, B.', 'Silva, C. T.']","[""Data visualization"", ""Visualization"", ""Semantics"", ""Usability"", ""Data analysis"", ""Task analysis"", ""Automobiles"", ""Natural language interface"", ""dataflow visualization system"", ""visual data exploration""]","Dataflow visualization systems enable flexible visual data exploration by allowing the user to construct a dataflow diagram that composes query and visualization modules to specify system functionality. However learning dataflow diagram usage presents overhead that often discourages the user. In this work we design FlowSense, a natural language interface for dataflow visualization systems that utilizes state-of-the-art natural language processing techniques to assist dataflow diagram construction. FlowSense employs a semantic parser with special utterance tagging and special utterance placeholders to generalize to different datasets and dataflow diagrams. It explicitly presents recognized dataset and diagram special utterances to the user for dataflow context awareness. With FlowSense the user can expand and adjust dataflow diagrams more conveniently via plain English. We apply FlowSense to the VisFlow subset-flow visualization system to enhance its usability. We evaluate FlowSense by one case study with domain experts on a real-world data analysis problem and a formal user study."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934538,JOUR,Data Changes Everything: Challenges and Opportunities in Data Visualization Design Handoff,IEEE Transactions on Visualization and Computer Graphics,12,22,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934538,"['Walny, J.', 'Frisson, C.', 'West, M.', 'Kosminsky, D.', 'Knudsen, S.', 'Carpendale, S.', 'Willett, W.']","[""Data visualization"", ""Tools"", ""Collaboration"", ""Design tools"", ""Software"", ""Task analysis"", ""Information visualization"", ""design handoff"", ""data mapping"", ""design process""]","Complex data visualization design projects often entail collaboration between people with different visualization-related skills. For example, many teams include both designers who create new visualization designs and developers who implement the resulting visualization software. We identify gaps between data characterization tools, visualization design tools, and development platforms that pose challenges for designer-developer teams working to create new data visualizations. While it is common for commercial interaction design tools to support collaboration between designers and developers, creating data visualizations poses several unique challenges that are not supported by current tools. In particular, visualization designers must characterize and build an understanding of the underlying data, then specify layouts, data encodings, and other data-driven parameters that will be robust across many different data values. In larger teams, designers must also clearly communicate these mappings and their dependencies to developers, clients, and other collaborators. We report observations and reflections from five large multidisciplinary visualization design projects and highlight six data-specific visualization challenges for design specification and handoff. These challenges include adapting to changing data, anticipating edge cases in data, understanding technical challenges, articulating data-dependent interactions, communicating data mappings, and preserving the integrity of data mappings across iterations. Based on these observations, we identify opportunities for future tools for prototyping, testing, and communicating data-driven designs, which might contribute to more successful and collaborative data visualization design."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934312,JOUR,InSituNet: Deep Image Synthesis for Parameter Space Exploration of Ensemble Simulations,IEEE Transactions on Visualization and Computer Graphics,23,33,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934312,"['He, W.', 'Wang, J.', 'Guo, H.', 'Wang, K.', 'Shen, H.', 'Raj, M.', 'Nashed, Y. S. G.', 'Peterka, T.']","[""Data visualization"", ""Data models"", ""Visualization"", ""Analytical models"", ""Space exploration"", ""Deep learning"", ""Image synthesis"", ""In situ visualization"", ""ensemble visualization"", ""parameter space exploration"", ""deep learning"", ""image synthesis""]","We propose InSituNet, a deep learning based surrogate model to support parameter space exploration for ensemble simulations that are visualized in situ. In situ visualization, generating visualizations at simulation time, is becoming prevalent in handling large-scale simulations because of the I/O and storage constraints. However, in situ visualization approaches limit the flexibility of post-hoc exploration because the raw simulation data are no longer available. Although multiple image-based approaches have been proposed to mitigate this limitation, those approaches lack the ability to explore the simulation parameters. Our approach allows flexible exploration of parameter space for large-scale ensemble simulations by taking advantage of the recent advances in deep learning. Specifically, we design InSituNet as a convolutional regression model to learn the mapping from the simulation and visualization parameters to the visualization results. With the trained model, users can generate new images for different simulation parameters under various visualization settings, which enables in-depth analysis of the underlying ensemble simulations. We demonstrate the effectiveness of InSituNet in combustion, cosmology, and ocean simulations through quantitative and qualitative evaluations."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934591,JOUR,NNVA: Neural Network Assisted Visual Analysis of Yeast Cell Polarization Simulation,IEEE Transactions on Visualization and Computer Graphics,34,44,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934591,"['Hazarika, S.', 'Li, H.', 'Wang, K.', 'Shen, H.', 'Chou, C.']","[""Analytical models"", ""Computational modeling"", ""Visualization"", ""Biological system modeling"", ""Neural networks"", ""Machine learning"", ""Data models"", ""Surrogate modeling"", ""Neural networks"", ""Computational biology"", ""Visual analysis"", ""Parameter analysis""]","Complex computational models are often designed to simulate real-world physical phenomena in many scientific disciplines. However, these simulation models tend to be computationally very expensive and involve a large number of simulation input parameters, which need to be analyzed and properly calibrated before the models can be applied for real scientific studies. We propose a visual analysis system to facilitate interactive exploratory analysis of high-dimensional input parameter space for a complex yeast cell polarization simulation. The proposed system can assist the computational biologists, who designed the simulation model, to visually calibrate the input parameters by modifying the parameter values and immediately visualizing the predicted simulation outcome without having the need to run the original expensive simulation for every instance. Our proposed visual analysis system is driven by a trained neural network-based surrogate model as the backend analysis framework. In this work, we demonstrate the advantage of using neural networks as surrogate models for visual analysis by incorporating some of the recent advances in the field of uncertainty quantification, interpretability and explainability of neural network-based models. We utilize the trained network to perform interactive parameter sensitivity analysis of the original simulation as well as recommend optimal parameter configurations using the activation maximization framework of neural networks. We also facilitate detail analysis of the trained network to extract useful insights about the simulation model, learned by the network, during the training process. We performed two case studies, and discovered multiple new parameter configurations, which can trigger high cell polarization results in the original simulation model. We evaluated our results by comparing with the original simulation model outcomes as well as the findings from previous parameter analysis performed by our experts."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934251,JOUR,Supporting Analysis of Dimensionality Reduction Results with Contrastive Learning,IEEE Transactions on Visualization and Computer Graphics,45,55,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934251,"['Fujiwara, T.', 'Kwon, O.', 'Ma, K.']","[""Principal component analysis"", ""Task analysis"", ""Data visualization"", ""Visual analytics"", ""Dimensionality reduction"", ""Feature extraction"", ""Dimensionality reduction"", ""contrastive learning"", ""principal component analysis"", ""high-dimensional data"", ""visual analytics""]","Dimensionality reduction (DR) is frequently used for analyzing and visualizing high-dimensional data as it provides a good first glance of the data. However, to interpret the DR result for gaining useful insights from the data, it would take additional analysis effort such as identifying clusters and understanding their characteristics. While there are many automatic methods (e.g., density-based clustering methods) to identify clusters, effective methods for understanding a cluster's characteristics are still lacking. A cluster can be mostly characterized by its distribution of feature values. Reviewing the original feature values is not a straightforward task when the number of features is large. To address this challenge, we present a visual analytics method that effectively highlights the essential features of a cluster in a DR result. To extract the essential features, we introduce an enhanced usage of contrastive principal component analysis (cPCA). Our method, called ccPCA (contrasting clusters in PCA), can calculate each feature's relative contribution to the contrast between one cluster and other clusters. With ccPCA, we have created an interactive system including a scalable visualization of clusters' feature contributions. We demonstrate the effectiveness of our method and system with case studies using several publicly available datasets."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934619,JOUR,The What-If Tool: Interactive Probing of Machine Learning Models,IEEE Transactions on Visualization and Computer Graphics,56,65,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934619,"['Wexler, J.', 'Pushkarna, M.', 'Bolukbasi, T.', 'Wattenberg, M.', 'Viegas, F.', 'Wilson, J.']","[""Tools"", ""Data models"", ""Data visualization"", ""Analytical models"", ""Predictive models"", ""Machine learning"", ""Computational modeling"", ""Interactive Machine Learning"", ""Model Debugging"", ""Model Comparison""]","A key challenge in developing and deploying Machine Learning (ML) systems is understanding their performance across a wide range of inputs. To address this challenge, we created the What-If Tool, an open-source application that allows practitioners to probe, visualize, and analyze ML systems, with minimal coding. The What-If Tool lets practitioners test performance in hypothetical situations, analyze the importance of different data features, and visualize model behavior across multiple models and subsets of input data. It also lets practitioners measure systems according to multiple ML fairness metrics. We describe the design of the tool, and report on real-life usage at different organizations."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934593,JOUR,Understanding the Role of Alternatives in Data Analysis Practices,IEEE Transactions on Visualization and Computer Graphics,66,76,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934593,"['Liu, J.', 'Boukhelifa, N.', 'Eagan, J. R.']","[""Tools"", ""Data analysis"", ""Analytical models"", ""Computational modeling"", ""Interviews"", ""Task analysis"", ""Data models"", ""alternatives"", ""data workers"", ""data analysis"", ""data science"", ""sensemaking"", ""qualitative study""]","Data workers are people who perform data analysis activities as a part of their daily work but do not formally identify as data scientists. They come from various domains and often need to explore diverse sets of hypotheses and theories, a variety of data sources, algorithms, methods, tools, and visual designs. Taken together, we call these alternatives. To better understand and characterize the role of alternatives in their analyses, we conducted semi-structured interviews with 12 data workers with different types of expertise. We conducted four types of analyses to understand 1) why data workers explore alternatives; 2) the different notions of alternatives and how they fit into the sensemaking process; 3) the high-level processes around alternatives; and 4) their strategies to generate, explore, and manage those alternatives. We find that participants' diverse levels of domain and computational expertise, experience with different tools, and collaboration within their broader context play an important role in how they explore these alternatives. These findings call out the need for more attention towards a deeper understanding of alternatives and the need for better tools to facilitate the exploration, interpretation, and management of alternatives. Drawing upon these analyses and findings, we present a framework based on participants' 1) degree of attention, 2) abstraction level, and 3) analytic processes. We show how this framework can help understand how data workers consider such alternatives in their analyses and how tool designers might create tools to better support them."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934609,JOUR,VASABI: Hierarchical User Profiles for Interactive Visual User Behaviour Analytics,IEEE Transactions on Visualization and Computer Graphics,77,86,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934609,"['Nguyen, P. H.', 'Henkin, R.', 'Chen, S.', 'Andrienko, N.', 'Andrienko, G.', 'Thonnard, O.', 'Turkay, C.']","[""Task analysis"", ""Data visualization"", ""Feature extraction"", ""Visual analytics"", ""Analytical models"", ""Buildings"", ""hierarchical user profiles"", ""user behaviour analytics"", ""visual analytics"", ""cybersecurity""]","User behaviour analytics (UBA) systems offer sophisticated models that capture users' behaviour over time with an aim to identify fraudulent activities that do not match their profiles. Motivated by the challenges in the interpretation of UBA models, this paper presents a visual analytics approach to help analysts gain a comprehensive understanding of user behaviour at multiple levels, namely individual and group level. We take a user-centred approach to design a visual analytics framework supporting the analysis of collections of users and the numerous sessions of activities they conduct within digital applications. The framework is centred around the concept of hierarchical user profiles that are built based on features derived from sessions, as well as on user tasks extracted using a topic modelling approach to summarise and stratify user behaviour. We externalise a series of analysis goals and tasks, and evaluate our methods through use cases conducted with experts. We observe that with the aid of interactive visual hierarchical user profiles, analysts are able to conduct exploratory and investigative analysis effectively, and able to understand the characteristics of user behaviour to make informed decisions whilst evaluating suspicious users and activities."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934539,JOUR,Criteria for Rigor in Visualization Design Study,IEEE Transactions on Visualization and Computer Graphics,87,97,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934539,"['Meyer, M.', 'Dykes, J.']","[""Visualization"", ""Social sciences"", ""Data visualization"", ""Information systems"", ""Context"", ""Production"", ""design study"", ""relativism"", ""interpretivism"", ""knowledge construction"", ""qualitative research"", ""research through design""]","We develop a new perspective on research conducted through visualization design study that emphasizes design as a method of inquiry and the broad range of knowledge-contributions achieved through it as multiple, subjective, and socially constructed. From this interpretivist position we explore the nature of visualization design study and develop six criteria for rigor. We propose that rigor is established and judged according to the extent to which visualization design study research and its reporting are INFORMED, REFLEXIVE, ABUNDANT, PLAUSIBLE, RESONANT, and TRANSPARENT. This perspective and the criteria were constructed through a four-year engagement with the discourse around rigor and the nature of knowledge in social science, information systems, and design. We suggest methods from cognate disciplines that can support visualization researchers in meeting these criteria during the planning, execution, and reporting of design study. Through a series of deliberately provocative questions, we explore implications of this new perspective for design study research in visualization, concluding that as a discipline, visualization is not yet well positioned to embrace, nurture, and fully benefit from a rigorous, interpretivist approach to design study. The perspective and criteria we present are intended to stimulate dialogue and debate around the nature of visualization design study and the broader underpinnings of the discipline."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934788,JOUR,Data by Proxy -- Material Traces as Autographic Visualizations,IEEE Transactions on Visualization and Computer Graphics,98,108,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934788,"['Offenhuber, D.']","[""Data visualization"", ""Visualization"", ""Data models"", ""History"", ""Semiotics"", ""Ice"", ""Indexes"", ""Traces"", ""indexicality"", ""data physicalization"", ""proxy data sources"", ""data materiality""]","Information visualization limits itself, per definition, to the domain of symbolic information. This paper discusses arguments why the field should also consider forms of data that are not symbolically encoded, including physical traces and material indicators. Continuing a provocation presented by Pat Hanrahan in his 2004 IEEE Vis capstone address, this paper compares physical traces to visualizations and describes the techniques and visual practices for producing, revealing, and interpreting them. By contrasting information visualization with a speculative counter model of autographic visualization, this paper examines the design principles for material data. Autographic visualization addresses limitations of information visualization, such as the inability to directly reflect the material circumstances of data generation. The comparison between the two models allows probing the epistemic assumptions behind information visualization and uncovers linkages with the rich history of scientific visualization and trace reading. The paper begins by discussing the gap between data visualizations and their corresponding phenomena and proceeds by investigating how material visualizations can bridge this gap. It contextualizes autographic visualization with paradigms such as data physicalization and indexical visualization and grounds it in the broader theoretical literature of semiotics, science and technology studies (STS), and the history of scientific representation. The main section of the paper proposes a foundational design vocabulary for autographic visualization and offers examples of how citizen scientists already use autographic principles in their displays, which seem to violate the canonical principles of information visualization but succeed at fulfilling other rhetorical purposes in evidence construction. The paper concludes with a discussion of the limitations of autographic visualization, a roadmap for the empirical investigation of trace perception, and thoughts about how information visualization and autographic visualization techniques can contribute to each other."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934790,JOUR,Design by Immersion: A Transdisciplinary Approach to Problem-Driven Visualizations,IEEE Transactions on Visualization and Computer Graphics,109,118,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934790,"['Hall, K. W.', 'Bradley, A. J.', 'Hinrichs, U.', 'Huron, S.', 'Wood, J.', 'Collins, C.', 'Carpendale, S.']","[""Visualization"", ""Collaboration"", ""Cultural differences"", ""Data visualization"", ""Buildings"", ""Global communication"", ""Visualization"", ""problem-driven"", ""design studies"", ""collaboration"", ""methodology"", ""framework""]","While previous work exists on how to conduct and disseminate insights from problem-driven visualization projects and design studies, the literature does not address how to accomplish these goals in transdisciplinary teams in ways that advance all disciplines involved. In this paper we introduce and define a new methodological paradigm we call design by immersion, which provides an alternative perspective on problem-driven visualization work. Design by immersion embeds transdisciplinary experiences at the center of the visualization process by having visualization researchers participate in the work of the target domain (or domain experts participate in visualization research). Based on our own combined experiences of working on cross-disciplinary, problem-driven visualization projects, we present six case studies that expose the opportunities that design by immersion enables, including (1) exploring new domain-inspired visualization design spaces, (2) enriching domain understanding through personal experiences, and (3) building strong transdisciplinary relationships. Furthermore, we illustrate how the process of design by immersion opens up a diverse set of design activities that can be combined in different ways depending on the type of collaboration, project, and goals. Finally, we discuss the challenges and potential pitfalls of design by immersion."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934283,JOUR,What is Interaction for Data Visualization?,IEEE Transactions on Visualization and Computer Graphics,119,129,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934283,"['Dimara, E.', 'Perin, C.']","[""Data visualization"", ""Human computer interaction"", ""Pipelines"", ""Current measurement"", ""Tagging"", ""Visual analytics"", ""interaction"", ""visualization"", ""data"", ""definition"", ""human-computer interaction""]","Interaction is fundamental to data visualization, but what ""interaction@ means in the context of visualization is ambiguous and confusing. We argue that this confusion is due to a lack of consensual definition. To tackle this problem, we start by synthesizing an inclusive view of interaction in the visualization community @ including insights from information visualization, visual analytics and scientific visualization, as well as the input of both senior and junior visualization researchers. Once this view takes shape, we look at how interaction is defined in the field of human-computer interaction (HCI). By extracting commonalities and differences between the views of interaction in visualization and in HCI, we synthesize a definition of interaction for visualization. Our definition is meant to be a thinking tool and inspire novel and bolder interaction design practices. We hope that by better understanding what interaction in visualization is and what it can be, we will enrich the quality of interaction in visualization systems and empower those who use them."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934287,JOUR,Why Authors Don't Visualize Uncertainty,IEEE Transactions on Visualization and Computer Graphics,130,139,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934287,"['Hullman, J.']","[""Uncertainty"", ""Data visualization"", ""Visualization"", ""Interviews"", ""Decision making"", ""Media"", ""Adaptation models"", ""Uncertainty visualization"", ""graphical statistical inference"", ""visualization rhetoric""]","Clear presentation of uncertainty is an exception rather than rule in media articles, data-driven reports, and consumer applications, despite proposed techniques for communicating sources of uncertainty in data. This work considers, Why do so many visualization authors choose not to visualize uncertainty? I contribute a detailed characterization of practices, associations, and attitudes related to uncertainty communication among visualization authors, derived from the results of surveying 90 authors who regularly create visualizations for others as part of their work, and interviewing thirteen influential visualization designers. My results highlight challenges that authors face and expose assumptions and inconsistencies in beliefs about the role of uncertainty in visualization. In particular, a clear contradiction arises between authors' acknowledgment of the value of depicting uncertainty and the norm of omitting direct depiction of uncertainty. To help explain this contradiction, I present a rhetorical model of uncertainty omission in visualization-based communication. I also adapt a formal statistical model of how viewers judge the strength of a signal in a visualization to visualization-based communication, to argue that uncertainty communication necessarily reduces degrees of freedom in viewers' statistical inferences. I conclude with recommendations for how visualization research on uncertainty communication could better serve practitioners' current needs and values while deepening understanding of assumptions that reinforce uncertainty omission."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934620,JOUR,High-throughput feature extraction for measuring attributes of deforming open-cell foams,IEEE Transactions on Visualization and Computer Graphics,140,150,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934620,"['Petruzza, S.', 'Gyulassy, A.', 'Leventhal, S.', 'Baglino, J. J.', 'Czabaj, M.', 'Spear, A. D.', 'Pascucci, V.']","[""Ligaments"", ""Junctions"", ""Computed tomography"", ""Image coding"", ""Feature extraction"", ""Image segmentation"", ""Lattices"", ""Topological analysis"", ""foam"", ""features extraction"", ""feature tracking""]","Metallic open-cell foams are promising structural materials with applications in multifunctional systems such as biomedical implants, energy absorbers in impact, noise mitigation, and batteries. There is a high demand for means to understand and correlate the design space of material performance metrics to the material structure in terms of attributes such as density, ligament and node properties, void sizes, and alignments. Currently, X-ray Computed Tomography (CT) scans of these materials are segmented either manually or with skeletonization approaches that may not accurately model the variety of shapes present in nodes and ligaments, especially irregularities that arise from manufacturing, image artifacts, or deterioration due to compression. In this paper, we present a new workflow for analysis of open-cell foams that combines a new density measurement to identify nodal structures, and topological approaches to identify ligament structures between them. Additionally, we provide automated measurement of foam properties. We demonstrate stable extraction of features and time-tracking in an image sequence of a foam being compressed. Our approach allows researchers to study larger and more complex foams than could previously be segmented only manually, and enables the high-throughput analysis needed to predict future foam performance."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934256,JOUR,Progressive Wasserstein Barycenters of Persistence Diagrams,IEEE Transactions on Visualization and Computer Graphics,151,161,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934256,"['Vidal, J.', 'Budin, J.', 'Tierny, J.']","[""Data visualization"", ""Market research"", ""Approximation algorithms"", ""Clustering algorithms"", ""Measurement"", ""Time factors"", ""Uncertainty"", ""Topological data analysis"", ""scalar data"", ""ensemble data""]","This paper presents an efficient algorithm for the progressive approximation of Wasserstein barycenters of persistence diagrams, with applications to the visual analysis of ensemble data. Given a set of scalar fields, our approach enables the computation of a persistence diagram which is representative of the set, and which visually conveys the number, data ranges and saliences of the main features of interest found in the set. Such representative diagrams are obtained by computing explicitly the discrete Wasserstein barycenter of the set of persistence diagrams, a notoriously computationally intensive task. In particular, we revisit efficient algorithms for Wasserstein distance approximation [12], [51] to extend previous work on barycenter estimation [94]. We present a new fast algorithm, which progressively approximates the barycenter by iteratively increasing the computation accuracy as well as the number of persistent features in the output diagram. Such a progressivity drastically improves convergence in practice and allows to design an interruptible algorithm, capable of respecting computation time constraints. This enables the approximation of Wasserstein barycenters within interactive times. We present an application to ensemble clustering where we revisit the $k$-means algorithm to exploit our barycenters and compute, within execution time constraints, meaningful clusters of ensemble data along with their barycenter diagram. Extensive experiments on synthetic and real-life data sets report that our algorithm converges to barycenters that are qualitatively meaningful with regard to the applications, and quantitatively comparable to previous techniques, while offering an order of magnitude speedup when run until convergence (without time constraint). Our algorithm can be trivially parallelized to provide additional speedups in practice on standard workstations. We provide a lightweight C++ implementation of our approach that can be used to reproduce our results."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934338,JOUR,The Effect of Data Transformations on Scalar Field Topological Analysis of High-Order FEM Solutions,IEEE Transactions on Visualization and Computer Graphics,162,172,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934338,"['Jallepalli, A.', 'Levine, J. A.', 'Kirby, R. M.']","[""Data models"", ""Finite element analysis"", ""Tools"", ""Data visualization"", ""Level set"", ""Analytical models"", ""Transforms"", ""High-Order Finite Element Methods"", ""Filtering Techniques"", ""Scalar Field Visualization"", ""Topological Analysis""]","High-order finite element methods (HO-FEM) are gaining popularity in the simulation community due to their success in solving complex flow dynamics. There is an increasing need to analyze the data produced as output by these simulations. Simultaneously, topological analysis tools are emerging as powerful methods for investigating simulation data. However, most of the current approaches to topological analysis have had limited application to HO-FEM simulation data for two reasons. First, the current topological tools are designed for linear data (polynomial degree one), but the polynomial degree of the data output by these simulations is typically higher (routinely up to polynomial degree six). Second, the simulation data and derived quantities of the simulation data have discontinuities at element boundaries, and these discontinuities do not match the input requirements for the topological tools. One solution to both issues is to transform the high-order data to achieve low-order, continuous inputs for topological analysis. Nevertheless, there has been little work evaluating the possible transformation choices and their downstream effect on the topological analysis. We perform an empirical study to evaluate two commonly used data transformation methodologies along with the recently introduced L-SIAC filter for processing high-order simulation data. Our results show diverse behaviors are possible. We offer some guidance about how best to consider a pipeline of topological analysis of HO-FEM simulations with the currently available implementations of topological analysis."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934257,JOUR,Toward Localized Topological Data Structures: Querying the Forest for the Tree,IEEE Transactions on Visualization and Computer Graphics,173,183,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934257,"['Klacansky, P.', 'Gyulassy, A.', 'Bremer, P.', 'Pascucci, V.']","[""Vegetation"", ""Forestry"", ""Feature extraction"", ""Data structures"", ""Scalability"", ""Acceleration"", ""Task analysis"", ""Merge tree"", ""parallel computation"", ""topology""]","Topological approaches to data analysis can answer complex questions about the number, connectivity, and scale of intrinsic features in scalar data. However, the global nature of many topological structures makes their computation challenging at scale, and thus often limits the size of data that can be processed. One key quality to achieving scalability and performance on modern architectures is data locality, i.e., a process operates on data that resides in a nearby memory system, avoiding frequent jumps in data access patterns. From this perspective, topological computations are particularly challenging because the implied data structures represent features that can span the entire data set, often requiring a global traversal phase that limits their scalability. Traditionally, expensive preprocessing is considered an acceptable trade-off as it accelerates all subsequent queries. Most published use cases, however, explore only a fraction of all possible queries, most often those returning small, local features. In these cases, much of the global information is not utilized, yet computing it dominates the overall response time. We address this challenge for merge trees, one of the most commonly used topological structures. In particular, we propose an alternative representation, the merge forest, a collection of local trees corresponding to regions in a domain decomposition. Local trees are connected by a bridge set that allows us to recover any necessary global information at query time. The resulting system couples (i) a preprocessing that scales linearly in practice with (ii) fast runtime queries that provide the same functionality as traditional queries of a global merge tree. We test the scalability of our approach on a shared-memory parallel computer and demonstrate how data structure locality enables the analysis of large data with an order of magnitude performance improvement over the status quo. Furthermore, a merge forest reduces the memory overhead compared to a global merge tree and enables the processing of data sets that are an order of magnitude larger than possible with previous algorithms."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934300,JOUR,GUIRO: User-Guided Matrix Reordering,IEEE Transactions on Visualization and Computer Graphics,184,194,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934300,"['Behrisch, M.', 'Schreck, T.', 'Pfister, H.']","[""Visualization"", ""Data visualization"", ""Indexes"", ""Topology"", ""Measurement"", ""Task analysis"", ""Partitioning algorithms"", ""Visual Analytics"", ""matrix"", ""black-box algorithms"", ""seriation"", ""ordering"", ""sorting"", ""steerable algorithm"", ""interaction"", ""2D projection""]","Matrix representations are one of the main established and empirically proven to be effective visualization techniques for relational (or network) data. However, matrices--similar to node-link diagrams--are most effective if their layout reveals the underlying data topology. Given the many developed algorithms, a practical problem arises: ""Which matrix reordering algorithm should I choose for my dataset at hand?@ To make matters worse, different reordering algorithms applied to the same dataset may let significantly different visual matrix patterns emerge. This leads to the question of trustworthiness and explainability of these fully automated, often heuristic, black-box processes. We present GUIRO, a Visual Analytics system that helps novices, network analysts, and algorithm designers to open the black-box. Users can investigate the usefulness and expressiveness of 70 accessible matrix reordering algorithms. For network analysts, we introduce a novel model space representation and two interaction techniques for a user-guided reordering of rows or columns, and especially groups thereof (submatrix reordering). These novel techniques contribute to the understanding of the global and local dataset topology. We support algorithm designers by giving them access to 16 reordering quality metrics and visual exploration means for comparing reordering implementations on a row/column permutation level. We evaluated GUIRO in a guided explorative user study with 12 subjects, a case study demonstrating its usefulness in a real-world scenario, and through an expert study gathering feedback on our design decisions. We found that our proposed methods help even inexperienced users to understand matrix patterns and allow a user-guided steering of reordering algorithms. GUIRO helps to increase the transparency of matrix reordering algorithms, thus helping a broad range of users to get a better insight into the complex reordering process, in turn supporting data and reordering algorithm insights."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934332,JOUR,LassoNet: Deep Lasso-Selection of 3D Point Clouds,IEEE Transactions on Visualization and Computer Graphics,195,204,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934332,"['Chen, Z.', 'Zeng, W.', 'Yang, Z.', 'Yu, L.', 'Fu, C.', 'Qu, H.']","[""Three-dimensional displays"", ""Two dimensional displays"", ""Deep learning"", ""Neural networks"", ""Shape"", ""Task analysis"", ""Visualization"", ""Point Clouds"", ""Lasso Selection"", ""Deep Learning""]","Selection is a fundamental task in exploratory analysis and visualization of 3D point clouds. Prior researches on selection methods were developed mainly based on heuristics such as local point density, thus limiting their applicability in general data. Specific challenges root in the great variabilities implied by point clouds (e.g., dense vs. sparse), viewpoint (e.g., occluded vs. non-occluded), and lasso (e.g., small vs. large). In this work, we introduce LassoNet, a new deep neural network for lasso selection of 3D point clouds, attempting to learn a latent mapping from viewpoint and lasso to point cloud regions. To achieve this, we couple user-target points with viewpoint and lasso information through 3D coordinate transform and naive selection, and improve the method scalability via an intention filtering and farthest point sampling. A hierarchical network is trained using a dataset with over 30K lasso-selection records on two different point cloud data. We conduct a formal user study to compare LassoNet with two state-of-the-art lasso-selection methods. The evaluations confirm that our approach improves the selection effectiveness and efficiency across different combinations of 3D point clouds, viewpoints, and lasso selections. Project Website: https://LassoNet.github.io"
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934255,JOUR,TSR-TVD: Temporal Super-Resolution for Time-Varying Data Analysis and Visualization,IEEE Transactions on Visualization and Computer Graphics,205,215,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934255,"['Han, J.', 'Wang, C.']","[""Gallium nitride"", ""Data visualization"", ""Deep learning"", ""Spatial resolution"", ""Training"", ""Generators"", ""Generative adversarial networks"", ""Time-varying data visualization"", ""super-resolution"", ""deep learning"", ""recurrent generative network""]","We present TSR-TVD, a novel deep learning framework that generates temporal super-resolution (TSR) of time-varying data (TVD) using adversarial learning. TSR-TVD is the first work that applies the recurrent generative network (RGN), a combination of the recurrent neural network (RNN) and generative adversarial network (GAN), to generate temporal high-resolution volume sequences from low-resolution ones. The design of TSR-TVD includes a generator and a discriminator. The generator takes a pair of volumes as input and outputs the synthesized intermediate volume sequence through forward and backward predictions. The discriminator takes the synthesized intermediate volumes as input and produces a score indicating the realness of the volumes. Our method handles multivariate data as well where the trained network from one variable is applied to generate TSR for another variable. To demonstrate the effectiveness of TSR-TVD, we show quantitative and qualitative results with several time-varying multivariate data sets and compare our method against standard linear interpolation and solutions solely based on RNN or CNN."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934806,JOUR,GenerativeMap: Visualization and Exploration of Dynamic Density Maps via Generative Learning Model,IEEE Transactions on Visualization and Computer Graphics,216,226,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934806,"['Chen, C.', 'Wang, C.', 'Bai, X.', 'Zhang, P.', 'Li, C.']","[""Data visualization"", ""Gallium nitride"", ""Spatiotemporal phenomena"", ""Pipelines"", ""Task analysis"", ""Interpolation"", ""Deep learning"", ""Density map"", ""deep learning"", ""spatiotemporal data"", ""generative model""]","The density map is widely used for data sampling, time-varying detection, ensemble representation, etc. The visualization of dynamic evolution is a challenging task when exploring spatiotemporal data. Many approaches have been provided to explore the variation of data patterns over time, which commonly need multiple parameters and preprocessing works. Image generation is a well-known topic in deep learning, and a variety of generating models have been promoted in recent years. In this paper, we introduce a general pipeline called GenerativeMap to extract dynamics of density maps by generating interpolation information. First, a trained generative model comprises an important part of our approach, which can generate nonlinear and natural results by implementing a few parameters. Second, a visual presentation is proposed to show the density change, which is combined with the level of detail and blue noise sampling for a better visual effect. Third, for dynamic visualization of large-scale density maps, we extend this approach to show the evolution in regions of interest, which costs less to overcome the drawback of the learning-based generative model. We demonstrate our method on different types of cases, and we evaluate and compare the approach from multiple aspects. The results help identify the effectiveness of our approach and confirm its applicability in different scenarios."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934547,JOUR,Facetto: Combining Unsupervised and Supervised Learning for Hierarchical Phenotype Analysis in Multi-Channel Image Data,IEEE Transactions on Visualization and Computer Graphics,227,237,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934547,"['Krueger, R.', 'Beyer, J.', 'Jang, W.', 'Kim, N. W.', 'Sokolov, A.', 'Sorger, P. K.', 'Pfister, H.']","[""Cancer"", ""Tools"", ""Visualization"", ""Rendering (computer graphics)"", ""Biomedical imaging"", ""Multiplexing"", ""Clustering"", ""Classification"", ""Visual Analysis"", ""Multiplex Tissue Imaging"", ""Digital Pathology"", ""Cancer Systems Biology""]","Facetto is a scalable visual analytics application that is used to discover single-cell phenotypes in high-dimensional multi-channel microscopy images of human tumors and tissues. Such images represent the cutting edge of digital histology and promise to revolutionize how diseases such as cancer are studied, diagnosed, and treated. Highly multiplexed tissue images are complex, comprising 109 or more pixels, 60-plus channels, and millions of individual cells. This makes manual analysis challenging and error-prone. Existing automated approaches are also inadequate, in large part, because they are unable to effectively exploit the deep knowledge of human tissue biology available to anatomic pathologists. To overcome these challenges, Facetto enables a semi-automated analysis of cell types and states. It integrates unsupervised and supervised learning into the image and feature exploration process and offers tools for analytical provenance. Experts can cluster the data to discover new types of cancer and immune cells and use clustering results to train a convolutional neural network that classifies new cells accordingly. Likewise, the output of classifiers can be clustered to discover aggregate patterns and phenotype subsets. We also introduce a new hierarchical approach to keep track of analysis steps and data subsets created by users; this assists in the identification of cell types. Users can build phenotype trees and interact with the resulting hierarchical structures of both high-dimensional feature and image spaces. We report on use-cases in which domain scientists explore various large-scale fluorescence imaging datasets. We demonstrate how Facetto assists users in steering the clustering and classification process, inspecting analysis results, and gaining new scientific insights into cancer biology."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934267,JOUR,ProtoSteer: Steering Deep Sequence Model with Prototypes,IEEE Transactions on Visualization and Computer Graphics,238,248,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934267,"['Ming, Y.', 'Xu, P.', 'Cheng, F.', 'Qu, H.', 'Ren, L.']","[""Prototypes"", ""Data visualization"", ""Data models"", ""Machine learning"", ""Computational modeling"", ""Predictive models"", ""Task analysis"", ""Sequence Data"", ""Explainable Artificial Intelligence (XAI)"", ""Recurrent Neural Networks (RNNs)"", ""Prototype Learning""]","Recently we have witnessed growing adoption of deep sequence models (e.g. LSTMs) in many application domains, including predictive health care, natural language processing, and log analysis. However, the intricate working mechanism of these models confines their accessibility to the domain experts. Their black-box nature also makes it a challenging task to incorporate domain-specific knowledge of the experts into the model. In ProtoSteer (Prototype Steering), we tackle the challenge of directly involving the domain experts to steer a deep sequence model without relying on model developers as intermediaries. Our approach originates in case-based reasoning, which imitates the common human problem-solving process of consulting past experiences to solve new problems. We utilize ProSeNet (Prototype Sequence Network), which learns a small set of exemplar cases (i.e., prototypes) from historical data. In ProtoSteer they serve both as an efficient visual summary of the original data and explanations of model decisions. With ProtoSteer the domain experts can inspect, critique, and revise the prototypes interactively. The system then incorporates user-specified prototypes and incrementally updates the model. We conduct extensive case studies and expert interviews in application domains including sentiment analysis on texts and predictive diagnostics based on vehicle fault logs. The results demonstrate that involvements of domain users can help obtain more interpretable models with concise prototypes while retaining similar accuracy."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934368,JOUR,Dynamic Nested Tracking Graphs,IEEE Transactions on Visualization and Computer Graphics,249,258,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934368,"['Lukasczyk, J.', 'Garth, C.', 'Weber, G. H.', 'Biedert, T.', 'Maciejewski, R.', 'Leitte, H.']","[""Databases"", ""Computational modeling"", ""Analytical models"", ""Motion pictures"", ""Vegetation"", ""Visual analytics"", ""Task analysis"", ""Topological Data Analysis"", ""Nested Tracking Graphs"", ""Image Databases"", ""Feature Tracking"", ""Post Hoc Visual Analytics""]","This work describes an approach for the interactive visual analysis of large-scale simulations, where numerous superlevel set components and their evolution are of primary interest. The approach first derives, at simulation runtime, a specialized Cinema database that consists of images of component groups, and topological abstractions. This database is processed by a novel graph operation-based nested tracking graph algorithm (GO-NTG) that dynamically computes NTGs for component groups based on size, overlap, persistence, and level thresholds. The resulting NTGs are in turn used in a feature-centered visual analytics framework to query specific database elements and update feature parameters, facilitating flexible post hoc analysis."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934310,JOUR,Extraction and Visual Analysis of Potential Vorticity Banners around the Alps,IEEE Transactions on Visualization and Computer Graphics,259,269,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934310,"['Bader, R.', 'Sprenger, M.', 'Ban, N.', 'Rudisuhli, S.', 'Schar, C.', 'Gunther, T.']","[""Feature extraction"", ""Visualization"", ""Prediction algorithms"", ""Three-dimensional displays"", ""Meteorology"", ""Two dimensional displays"", ""Cyclones"", ""Scientific Visualization"", ""potential vorticity"", ""meteorology"", ""feature extraction""]","Potential vorticity is among the most important scalar quantities in atmospheric dynamics. For instance, potential vorticity plays a key role in particularly strong wind peaks in extratropical cyclones and it is able to explain the occurrence of frontal rain bands. Potential vorticity combines the key quantities of atmospheric dynamics, namely rotation and stratification. Under suitable wind conditions elongated banners of potential vorticity appear in the lee of mountains. Their role in atmospheric dynamics has recently raised considerable interest in the meteorological community for instance due to their influence in aviation wind hazards and maritime transport. In order to support meteorologists and climatologists in the analysis of these structures, we developed an extraction algorithm and a visual exploration framework consisting of multiple linked views. For the extraction we apply a predictor-corrector algorithm that follows streamlines and realigns them with extremal lines of potential vorticity. Using the agglomerative hierarchical clustering algorithm, we group banners from different sources based on their proximity. To visually analyze the time-dependent banner geometry, we provide interactive overviews and enable the query for detail on demand, including the analysis of different time steps, potentially correlated scalar quantities, and the wind vector field. In particular, we study the relationship between relative humidity and the banners for their potential in indicating the development of precipitation. Working with our method, the collaborating meteorologists gained a deeper understanding of the three-dimensional processes, which may spur follow-up research in the future."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934314,JOUR,Multi-Scale Topological Analysis of Asymmetric Tensor Fields on Surfaces,IEEE Transactions on Visualization and Computer Graphics,270,279,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934314,"['Khan, F.', 'Roy, L.', 'Zhang, E.', 'Qu, B.', 'Hung, S.', 'Yeh, H.', 'Laramee, R. S.', 'Zhang, Y.']","[""Eigenvalues and eigenfunctions"", ""Manifolds"", ""Topology"", ""Two dimensional displays"", ""Data visualization"", ""Heuristic algorithms"", ""Tensor field visualization"", ""tensor field topology"", ""2D asymmetric tensor fields"", ""2D asymmetric tensor field topology"", ""eigenvalue graphs"", ""eigenvector graphs""]","Asymmetric tensor fields have found applications in many science and engineering domains, such as fluid dynamics. Recent advances in the visualization and analysis of 2D asymmetric tensor fields focus on pointwise analysis of the tensor field and effective visualization metaphors such as colors, glyphs, and hyperstreamlines. In this paper, we provide a novel multi-scale topological analysis framework for asymmetric tensor fields on surfaces. Our multi-scale framework is based on the notions of eigenvalue and eigenvector graphs. At the core of our framework are the identification of atomic operations that modify the graphs and the scale definition that guides the order in which the graphs are simplified to enable clarity and focus for the visualization of topological analysis on data of different sizes. We also provide efficient algorithms to realize these operations. Furthermore, we provide physical interpretation of these graphs. To demonstrate the utility of our system, we apply our multi-scale analysis to data in computational fluid dynamics."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934375,JOUR,Vector Field Topology of Time-Dependent Flows in a Steady Reference Frame,IEEE Transactions on Visualization and Computer Graphics,280,290,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934375,"['Rojo, I. B.', 'Gunther, T.']","[""Topology"", ""Feature extraction"", ""Optimization"", ""Bifurcation"", ""Three-dimensional displays"", ""Extraterrestrial measurements"", ""Two dimensional displays"", ""Scientific visualization"", ""unsteady flow"", ""vector field topology"", ""reference frame optimization""]","The topological analysis of unsteady vector fields remains to this day one of the largest challenges in flow visualization. We build up on recent work on vortex extraction to define a time-dependent vector field topology for 2D and 3D flows. In our work, we split the vector field into two components: a vector field in which the flow becomes steady, and the remaining ambient flow that describes the motion of topological elements (such as sinks, sources and saddles) and feature curves (vortex corelines and bifurcation lines). To this end, we expand on recent local optimization approaches by modeling spatially-varying deformations through displacement transformations from continuum mechanics. We compare and discuss the relationships with existing local and integration-based topology extraction methods, showing for instance that separatrices seeded from saddles in the optimal frame align with the integration-based streakline vector field topology. In contrast to the streakline-based approach, our method gives a complete picture of the topology for every time slice, including the steps near the temporal domain boundaries. With our work it now becomes possible to extract topological information even when only few time slices are available. We demonstrate the method in several analytical and numerically-simulated flows and discuss practical aspects, limitations and opportunities for future work."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934594,JOUR,Scalable Topological Data Analysis and Visualization for Evaluating Data-Driven Models in Scientific Applications,IEEE Transactions on Visualization and Computer Graphics,291,300,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934594,"['Liu, S.', 'Wang, D.', 'Maljovec, D.', 'Anirudh, R.', 'Thiagarajan, J. J.', 'Jacobs, S. A.', 'Essen, B. C. Van', 'Hysom, D.', 'Yeom, J.', 'Gaffney, J.', 'Peterson, L.', 'Robinson, P. B.', 'Bhatia, H.', 'Pascucci, V.', 'Spears, B. K.', 'Bremer, P.']","[""Computational modeling"", ""Data visualization"", ""Data analysis"", ""Analytical models"", ""Topology"", ""Physics"", ""Predictive models"", ""Model Evaluation"", ""Deep Learning"", ""High-Dimensional Space"", ""Topological Data Analysis"", ""Inertial Confinement Fusion""]","With the rapid adoption of machine learning techniques for large-scale applications in science and engineering comes the convergence of two grand challenges in visualization. First, the utilization of black box models (e.g., deep neural networks) calls for advanced techniques in exploring and interpreting model behaviors. Second, the rapid growth in computing has produced enormous datasets that require techniques that can handle millions or more samples. Although some solutions to these interpretability challenges have been proposed, they typically do not scale beyond thousands of samples, nor do they provide the high-level intuition scientists are looking for. Here, we present the first scalable solution to explore and analyze high-dimensional functions often encountered in the scientific data analysis pipeline. By combining a new streaming neighborhood graph construction, the corresponding topology computation, and a novel data aggregation scheme, namely topology aware datacubes, we enable interactive exploration of both the topological and the geometric aspect of high-dimensional data. Following two use cases from high-energy-density (HED) physics and computational biology, we demonstrate how these capabilities have led to crucial new insights in both applications."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934400,JOUR,"Biased Average Position Estimates in Line and Bar Graphs: Underestimation, Overestimation, and Perceptual Pull",IEEE Transactions on Visualization and Computer Graphics,301,310,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934400,"['Xiong, C.', 'Ceja, C. R.', 'Ludwig, C. J. H.', 'Franconeri, S.']","[""Bars"", ""Data visualization"", ""Visualization"", ""Encoding"", ""Systematics"", ""Image color analysis"", ""Task analysis"", ""Perceptual biases"", ""perception and cognition"", ""cue combination"", ""bar graphs"", ""line graphs"", ""position estimation""]","In visual depictions of data, position (i.e., the vertical height of a line or a bar) is believed to be the most precise way to encode information compared to other encodings (e.g., hue). Not only are other encodings less precise than position, but they can also be prone to systematic biases (e.g., color category boundaries can distort perceived differences between hues). By comparison, position's high level of precision may seem to protect it from such biases. In contrast, across three empirical studies, we show that while position may be a precise form of data encoding, it can also produce systematic biases in how values are visually encoded, at least for reports of average position across a short delay. In displays with a single line or a single set of bars, reports of average positions were significantly biased, such that line positions were underestimated and bar positions were overestimated. In displays with multiple data series (i.e., multiple lines and/or sets of bars), this systematic bias still persisted. We also observed an effect of ""perceptual pull@, where the average position estimate for each series was 'pulled' toward the other. These findings suggest that, although position may still be the most precise form of visual data encoding, it can also be systematically biased."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934801,JOUR,Measures of the Benefit of Direct Encoding of Data Deltas for Data Pair Relation Perception,IEEE Transactions on Visualization and Computer Graphics,311,320,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934801,"['Nothelfer, C.', 'Franconeri, S.']","[""Encoding"", ""Task analysis"", ""Bars"", ""Visualization"", ""Data visualization"", ""Time factors"", ""Information visualization"", ""marks"", ""perception"", ""attention"", ""visual comparison"", ""visual search"", ""aggregation""]","The power of data visualization is not to convey absolute values of individual data points, but to allow the exploration of relations (increases or decreases in a data value) among them. One approach to highlighting these relations is to explicitly encode the numeric differences (deltas) between data values. Because this approach removes the context of the individual data values, it is important to measure how much of a performance improvement it actually offers, especially across differences in encodings and tasks, to ensure that it is worth adding to a visualization design. Across 3 different tasks, we measured the increase in visual processing efficiency for judging the relations between pairs of data values, from when only the values were shown, to when the deltas between the values were explicitly encoded, across position and length visual feature encodings (and slope encodings in Experiments 1 & 2). In Experiment 1, the participant's task was to locate a pair of data values with a given relation (e.g., Find the 'small bar to the left of a tall bar' pair) among pairs of the opposite relation, and we measured processing efficiency from the increase in response times as the number of pairs increased. In Experiment 2, the task was to judge which of two relation types was more prevalent in a briefly presented display of 10 data pairs (e.g., Are there more 'small bar to the left of a tall bar' pairs or more 'tall bar to the left of a small bar' pairs?). In the final experiment, the task was to estimate the average delta within a briefly presented display of 6 data pairs (e.g., What is the average bar height difference across all 'small bar to the left of a tall bar' pairs?). Across all three experiments, visual processing of relations between data value pairs was significantly better when directly encoded as deltas rather than implicitly between individual data points, and varied substantially depending on the task (improvement ranged from 25% to 95%). Considering the ubiquity of bar charts and dot plots, relation perception for individual data values is highly inefficient, and confirms the need for alternative designs that provide not only absolute values, but also direct encoding of critical relationships between those values."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934208,JOUR,Evaluating Perceptual Bias During Geometric Scaling of Scatterplots,IEEE Transactions on Visualization and Computer Graphics,321,331,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934208,"['Wei, Y.', 'Mei, H.', 'Zhao, Y.', 'Zhou, S.', 'Lin, B.', 'Jiang, H.', 'Chen, W.']","[""Visualization"", ""Correlation"", ""Data analysis"", ""Encoding"", ""Measurement"", ""Portable computers"", ""Mobile handsets"", ""Evaluation"", ""scatterplot"", ""geometric scaling"", ""bias"", ""perceptual consistency""]","Scatterplots are frequently scaled to fit display areas in multi-view and multi-device data analysis environments. A common method used for scaling is to enlarge or shrink the entire scatterplot together with the inside points synchronously and proportionally. This process is called geometric scaling. However, geometric scaling of scatterplots may cause a perceptual bias, that is, the perceived and physical values of visual features may be dissociated with respect to geometric scaling. For example, if a scatterplot is projected from a laptop to a large projector screen, then observers may feel that the scatterplot shown on the projector has fewer points than that viewed on the laptop. This paper presents an evaluation study on the perceptual bias of visual features in scatterplots caused by geometric scaling. The study focuses on three fundamental visual features (i.e., numerosity, correlation, and cluster separation) and three hypotheses that are formulated on the basis of our experience. We carefully design three controlled experiments by using well-prepared synthetic data and recruit participants to complete the experiments on the basis of their subjective experience. With a detailed analysis of the experimental results, we obtain a set of instructive findings. First, geometric scaling causes a bias that has a linear relationship with the scale ratio. Second, no significant difference exists between the biases measured from normally and uniformly distributed scatterplots. Third, changing the point radius can correct the bias to a certain extent. These findings can be used to inspire the design decisions of scatterplots in various scenarios."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934286,JOUR,Toward Objective Evaluation of Working Memory in Visualizations: A Case Study Using Pupillometry and a Dual-Task Paradigm,IEEE Transactions on Visualization and Computer Graphics,332,342,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934286,"['Padilla, L. M. K.', 'Castro, S. C.', 'Quinan, P. S.', 'Ruginski, I. T.', 'Creem-Regehr, S. H.']","[""Task analysis"", ""Data visualization"", ""Memory management"", ""Visualization"", ""Sociology"", ""Statistics"", ""Cognitive science"", ""Working Memory"", ""Cognitive Effort"", ""Evaluation Methods"", ""Pupillometry"", ""Geographic/Geospatial Visualization"", ""Quantitative Evaluation""]","Cognitive science has established widely used and validated procedures for evaluating working memory in numerous applied domains, but surprisingly few studies have employed these methodologies to assess claims about the impacts of visualizations on working memory. The lack of information visualization research that uses validated procedures for measuring working memory may be due, in part, to the absence of cross-domain methodological guidance tailored explicitly to the unique needs of visualization research. This paper presents a set of clear, practical, and empirically validated methods for evaluating working memory during visualization tasks and provides readers with guidance in selecting an appropriate working memory evaluation paradigm. As a case study, we illustrate multiple methods for evaluating working memory in a visual-spatial aggregation task with geospatial data. The results show that the use of dual-task experimental designs (simultaneous performance of several tasks compared to single-task performance) and pupil dilation can reveal working memory demands associated with task difficulty and dual-tasking. In a dual-task experimental design, measures of task completion times and pupillometry revealed the working memory demands associated with both task difficulty and dual-tasking. Pupillometry demonstrated that participants' pupils were significantly larger when they were completing a more difficult task and when multitasking. We propose that researchers interested in the relative differences in working memory between visualizations should consider a converging methods approach, where physiological measures and behavioral measures of working memory are employed to generate a rich evaluation of visualization effort."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934797,JOUR,VisTA: Integrating Machine Intelligence with Visualization to Support the Investigation of Think-Aloud Sessions,IEEE Transactions on Visualization and Computer Graphics,343,352,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934797,"['Fan, M.', 'Wu, K.', 'Zhao, J.', 'Li, Y.', 'Wei, W.', 'Truong, K. N.']","[""Usability"", ""Visual analytics"", ""Tools"", ""Machine intelligence"", ""Feature extraction"", ""Machine learning"", ""Think-aloud"", ""visual analytics"", ""machine intelligence"", ""user study"", ""usability problems"", ""session review behavior"", ""UX practices""]","Think-aloud protocols are widely used by user experience (UX) practitioners in usability testing to uncover issues in user interface design. It is often arduous to analyze large amounts of recorded think-aloud sessions and few UX practitioners have an opportunity to get a second perspective during their analysis due to time and resource constraints. Inspired by the recent research that shows subtle verbalization and speech patterns tend to occur when users encounter usability problems, we take the first step to design and evaluate an intelligent visual analytics tool that leverages such patterns to identify usability problem encounters and present them to UX practitioners to assist their analysis. We first conducted and recorded think-aloud sessions, and then extracted textual and acoustic features from the recordings and trained machine learning (ML) models to detect problem encounters. Next, we iteratively designed and developed a visual analytics tool, VisTA, which enables dynamic investigation of think-aloud sessions with a timeline visualization of ML predictions and input features. We conducted a between-subjects laboratory study to compare three conditions, i.e., VisTA, VisTASimple (no visualization of the ML's input features), and Baseline (no ML information at all), with 30 UX professionals. The findings show that UX professionals identified more problem encounters when using VisTA than Baseline by leveraging the problem visualization as an overview, anticipations, and anchors as well as the feature visualization as a means to understand what ML considers and omits. Our findings also provide insights into how they treated ML, dealt with (dis)agreement with ML, and reviewed the videos (i.e., play, pause, and rewind)."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934264,JOUR,"The Validity, Generalizability and Feasibility of Summative Evaluation Methods in Visual Analytics",IEEE Transactions on Visualization and Computer Graphics,353,363,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934264,"['Khayat, M.', 'Karimzadeh, M.', 'Ebert, D. S.', 'Ghafoor, A.']","[""Taxonomy"", ""Visual analytics"", ""Measurement"", ""Usability"", ""Guidelines"", ""Focusing"", ""Summative evaluation"", ""usefulness"", ""evaluation process"", ""taxonomy"", ""visual analytics""]","Many evaluation methods have been used to assess the usefulness of Visual Analytics (VA) solutions. These methods stem from a variety of origins with different assumptions and goals, which cause confusion about their proofing capabilities. Moreover, the lack of discussion about the evaluation processes may limit our potential to develop new evaluation methods specialized for VA. In this paper, we present an analysis of evaluation methods that have been used to summatively evaluate VA solutions. We provide a survey and taxonomy of the evaluation methods that have appeared in the VAST literature in the past two years. We then analyze these methods in terms of validity and generalizability of their findings, as well as the feasibility of using them. We propose a new metric called summative quality to compare evaluation methods according to their ability to prove usefulness, and make recommendations for selecting evaluation methods based on their summative quality in the VA domain."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934397,JOUR,A Comparative Evaluation of Animation and Small Multiples for Trend Visualization on Mobile Phones,IEEE Transactions on Visualization and Computer Graphics,364,374,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934397,"['Brehmer, M.', 'Lee, B.', 'Isenberg, P.', 'Choe, E. K.']","[""Data visualization"", ""Animation"", ""Task analysis"", ""Market research"", ""Mobile handsets"", ""Trajectory"", ""Mobile applications"", ""Evaluation"", ""graphical perception"", ""mobile phones"", ""trend visualization"", ""animation"", ""small multiples"", ""crowdsourcing""]","We compare the efficacy of animated and small multiples variants of scatterplots on mobile phones for comparing trends in multivariate datasets. Visualization is increasingly prevalent in mobile applications and mobile-first websites, yet there is little prior visualization research dedicated to small displays. In this paper, we build upon previous experimental research carried out on larger displays that assessed animated and non-animated variants of scatterplots. Incorporating similar experimental stimuli and tasks, we conducted an experiment where 96 crowdworker participants performed nine trend comparison tasks using their mobile phones. We found that those using a small multiples design consistently completed tasks in less time, albeit with slightly less confidence than those using an animated design. The accuracy results were more task-dependent, and we further interpret our results according to the characteristics of the individual tasks, with a specific focus on the trajectories of target and distractor data items in each task. We identify cases that appear to favor either animation or small multiples, providing new questions for further experimental research and implications for visualization design on mobile devices. Lastly, we provide a reflection on our evaluation methodology."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934807,JOUR,A Comparison of Visualizations for Identifying Correlation over Space and Time,IEEE Transactions on Visualization and Computer Graphics,375,385,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934807,"['Pena-Araya, V.', 'Pietriga, E.', 'Bezerianos, A.']","[""Data visualization"", ""Visualization"", ""Encoding"", ""Correlation"", ""Task analysis"", ""Animation"", ""Shape"", ""geo-temporal data"", ""bivariate maps"", ""correlation"", ""controlled study"", ""bar chart"", ""Dorling cartogram"", ""small multiples""]","Observing the relationship between two or more variables over space and time is essential in many domains. For instance, looking, for different countries, at the evolution of both the life expectancy at birth and the fertility rate will give an overview of their demographics. The choice of visual representation for such multivariate data is key to enabling analysts to extract patterns and trends. Prior work has compared geo-temporal visualization techniques for a single thematic variable that evolves over space and time, or for two variables at a specific point in time. But how effective visualization techniques are at communicating correlation between two variables that evolve over space and time remains to be investigated. We report on a study comparing three techniques that are representative of different strategies to visualize geo-temporal multivariate data: either juxtaposing all locations for a given time step, or juxtaposing all time steps for a given location; and encoding thematic attributes either using symbols overlaid on top of map features, or using visual channels of the map features themselves. Participants performed a series of tasks that required them to identify if two variables were correlated over time and if there was a pattern in their evolution. Tasks varied in granularity for both dimensions: time (all time steps, a subrange of steps, one step only) and space (all locations, locations in a subregion, one location only). Our results show that a visualization's effectiveness depends strongly on the task to be carried out. Based on these findings we present a set of design guidelines about geo-temporal visualization techniques for communicating correlation."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934288,JOUR,Common Fate for Animated Transitions in Visualization,IEEE Transactions on Visualization and Computer Graphics,386,396,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934288,"['Chalbi, A.', 'Ritchie, J.', 'Park, D.', 'Choi, J.', 'Roussel, N.', 'Elmqvist, N.', 'Chevalier, F.']","[""Visualization"", ""Animation"", ""Data visualization"", ""Psychology"", ""Dynamics"", ""Visual perception"", ""Trajectory"", ""Gestalt laws"", ""common fate"", ""animated transitions"", ""evaluation"", ""motion""]","The Law of Common Fate from Gestalt psychology states that visual objects moving with the same velocity along parallel trajectories will be perceived by a human observer as grouped. However, the concept of common fate is much broader than mere velocity; in this paper we explore how common fate results from coordinated changes in luminance and size. We present results from a crowdsourced graphical perception study where we asked workers to make perceptual judgments on a series of trials involving four graphical objects under the influence of conflicting static and dynamic visual factors (position, size and luminance) used in conjunction. Our results yield the following rankings for visual grouping: motion > (dynamic luminance, size, luminance); dynamic size > (dynamic luminance, position); and dynamic luminance > size. We also conducted a follow-up experiment to evaluate the three dynamic visual factors in a more ecologically valid setting, using both a Gapminder-like animated scatterplot and a thematic map of election data. The results indicate that in practice the relative grouping strengths of these factors may depend on various parameters including the visualization characteristics and the underlying data. We discuss design implications for animated transitions in data visualization."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934243,JOUR,CourtTime: Generating Actionable Insights into Tennis Matches Using Visual Analytics,IEEE Transactions on Visualization and Computer Graphics,397,406,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934243,"['Polk, T.', 'Jackle, D.', 'HauSler, J.', 'Yang, J.']","[""Sports"", ""Visual analytics"", ""Spatial databases"", ""Games"", ""Data visualization"", ""Tracking"", ""Visual analytics"", ""tennis analysis"", ""sports analytics"", ""spatio-temporal analysis""]","Tennis players and coaches of all proficiency levels seek to understand and improve their play. Summary statistics alone are inadequate to provide the insights players need to improve their games. Spatio-temporal data capturing player and ball movements is likely to provide the actionable insights needed to identify player strengths, weaknesses, and strategies. To fully utilize this spatio-temporal data, we need to integrate it with domain-relevant context meta-data. In this paper, we propose CourtTime, a novel approach to perform data-driven visual analysis of individual tennis matches. Our visual approach introduces a novel visual metaphor, namely 1@D Space-Time Charts that enable the analysis of single points at a glance based on small multiples. We also employ user-driven sorting and clustering techniques and a layout technique that aligns the last few shots in a point to facilitate shot pattern discovery. We discuss the usefulness of CourtTime via an extensive case study and report on feedback from an amateur tennis player and three tennis coaches."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934630,JOUR,Tac-Simur: Tactic-based Simulative Visual Analytics of Table Tennis,IEEE Transactions on Visualization and Computer Graphics,407,417,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934630,"['Wang, J.', 'Zhao, K.', 'Deng, D.', 'Cao, A.', 'Xie, X.', 'Zhou, Z.', 'Zhang, H.', 'Wu, Y.']","[""Sports"", ""Visual analytics"", ""Analytical models"", ""Markov processes"", ""Games"", ""Predictive models"", ""Numerical models"", ""Simulative Visual Analytics"", ""Table Tennis"", ""Design Study""]","Simulative analysis in competitive sports can provide prospective insights, which can help improve the performance of players in future matches. However, adequately simulating the complex competition process and effectively explaining the simulation result to domain experts are typically challenging. This work presents a design study to address these challenges in table tennis. We propose a well-established hybrid second-order Markov chain model to characterize and simulate the competition process in table tennis. Compared with existing methods, our approach is the first to support the effective simulation of tactics, which represent high-level competition strategies in table tennis. Furthermore, we introduce a visual analytics system called Tac-Simur based on the proposed model for simulative visual analytics. Tac-Simur enables users to easily navigate different players and their tactics based on their respective performance in matches to identify the player and the tactics of interest for further analysis. Then, users can utilize the system to interactively explore diverse simulation tasks and visually explain the simulation results. The effectiveness and usefulness of this work are demonstrated by two case studies, in which domain experts utilize Tac-Simur to find interesting and valuable insights. The domain experts also provide positive feedback on the usability of Tac-Simur. Our work can be extended to other similar sports such as tennis and badminton."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934433,JOUR,An Incremental Dimensionality Reduction Method for Visualizing Streaming Multidimensional Data,IEEE Transactions on Visualization and Computer Graphics,418,428,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934433,"['Fujiwara, T.', 'Chou, J.', 'Shilpika, S.', 'Xu, P.', 'Ren, L.', 'Ma, K.']","[""Data visualization"", ""Layout"", ""Principal component analysis"", ""Dimensionality reduction"", ""Visual analytics"", ""Computational efficiency"", ""Animation"", ""Dimensionality reduction"", ""principal component analysis"", ""streaming data"", ""uncertainty"", ""visual analytics""]","Dimensionality reduction (DR) methods are commonly used for analyzing and visualizing multidimensional data. However, when data is a live streaming feed, conventional DR methods cannot be directly used because of their computational complexity and inability to preserve the projected data positions at previous time points. In addition, the problem becomes even more challenging when the dynamic data records have a varying number of dimensions as often found in real-world applications. This paper presents an incremental DR solution. We enhance an existing incremental PCA method in several ways to ensure its usability for visualizing streaming multidimensional data. First, we use geometric transformation and animation methods to help preserve a viewer's mental map when visualizing the incremental results. Second, to handle data dimension variants, we use an optimization method to estimate the projected data positions, and also convey the resulting uncertainty in the visualization. We demonstrate the effectiveness of our design with two case studies using real-world datasets."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934209,JOUR,Selection Bias Tracking and Detailed Subset Comparison for High-Dimensional Data,IEEE Transactions on Visualization and Computer Graphics,429,439,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934209,"['Borland, D.', 'Wang, W.', 'Zhang, J.', 'Shrestha, J.', 'Gotz, D.']","[""Data visualization"", ""Visual analytics"", ""Encoding"", ""Tools"", ""Complexity theory"", ""Medical diagnostic imaging"", ""High-dimensional visualization"", ""visual analytics"", ""cohort selection"", ""medical informatics"", ""selection bias""]","The collection of large, complex datasets has become common across a wide variety of domains. Visual analytics tools increasingly play a key role in exploring and answering complex questions about these large datasets. However, many visualizations are not designed to concurrently visualize the large number of dimensions present in complex datasets (e.g. tens of thousands of distinct codes in an electronic health record system). This fact, combined with the ability of many visual analytics systems to enable rapid, ad-hoc specification of groups, or cohorts, of individuals based on a small subset of visualized dimensions, leads to the possibility of introducing selection bias@when the user creates a cohort based on a specified set of dimensions, differences across many other unseen dimensions may also be introduced. These unintended side effects may result in the cohort no longer being representative of the larger population intended to be studied, which can negatively affect the validity of subsequent analyses. We present techniques for selection bias tracking and visualization that can be incorporated into high-dimensional exploratory visual analytics systems, with a focus on medical data with existing data hierarchies. These techniques include: (1) tree-based cohort provenance and visualization, including a user-specified baseline cohort that all other cohorts are compared against, and visual encoding of cohort ""drift@, which indicates where selection bias may have occurred, and (2) a set of visualizations, including a novel icicle-plot based visualization, to compare in detail the per-dimension differences between the baseline and a user-specified focus cohort. These techniques are integrated into a medical temporal event sequence visual analytics tool. We present example use cases and report findings from domain expert user interviews."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934661,JOUR,Visual Analysis of High-Dimensional Event Sequence Data via Dynamic Hierarchical Aggregation,IEEE Transactions on Visualization and Computer Graphics,440,450,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934661,"['Gotz, D.', 'Zhang, J.', 'Wang, W.', 'Shrestha, J.', 'Borland, D.']","[""Data visualization"", ""Visual analytics"", ""Runtime"", ""Sequences"", ""Layout"", ""Navigation"", ""Temporal event sequence visualization"", ""visual analytics"", ""hierarchical aggregation"", ""medical informatics""]","Temporal event data are collected across a broad range of domains, and a variety of visual analytics techniques have been developed to empower analysts working with this form of data. These techniques generally display aggregate statistics computed over sets of event sequences that share common patterns. Such techniques are often hindered, however, by the high-dimensionality of many real-world event sequence datasets which can prevent effective aggregation. A common coping strategy for this challenge is to group event types together prior to visualization, as a pre-process, so that each group can be represented within an analysis as a single event type. However, computing these event groupings as a pre-process also places significant constraints on the analysis. This paper presents a new visual analytics approach for dynamic hierarchical dimension aggregation. The approach leverages a predefined hierarchy of dimensions to computationally quantify the informativeness, with respect to a measure of interest, of alternative levels of grouping within the hierarchy at runtime. This information is then interactively visualized, enabling users to dynamically explore the hierarchy to select the most appropriate level of grouping to use at any individual step within an analysis. Key contributions include an algorithm for interactively determining the most informative set of event groupings for a specific analysis context, and a scented scatter-plus-focus visualization design with an optimization-based layout algorithm that supports interactive hierarchical exploration of alternative event type groupings. We apply these techniques to high-dimensional event sequence data from the medical domain and report findings from domain expert interviews."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934804,JOUR,Construct-A-Vis: Exploring the Free-Form Visualization Processes of Children,IEEE Transactions on Visualization and Computer Graphics,451,460,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934804,"['Bishop, F.', 'Zagermann, J.', 'Pfeil, U.', 'Sanderson, G.', 'Reiterer, H.', 'Hinrichs, U.']","[""Data visualization"", ""Visualization"", ""Tools"", ""Education"", ""Problem-solving"", ""Collaboration"", ""Buildings"", ""Visualization in Education"", ""Visualization with Children"", ""Qualitative Evaluation"", ""Visualization System and Toolkit Design""]","Building data analysis skills is part of modern elementary school curricula. Recent research has explored how to facilitate children's understanding of visual data representations through completion exercises which highlight links between concrete and abstract mappings. This approach scaffolds visualization activities by presenting a target visualization to children. But how can we engage children in more free-form visual data mapping exercises that are driven by their own mapping ideas? How can we scaffold a creative exploration of visualization techniques and mapping possibilities? We present Construct-A-Vis, a tablet-based tool designed to explore the feasibility of free-form and constructive visualization activities with elementary school children. Construct-A-Vis provides adjustable levels of scaffolding visual mapping processes. It can be used by children individually or as part of collaborative activities. Findings from a study with elementary school children using Construct-A-Vis individually and in pairs highlight the potential of this free-form constructive approach, as visible in children's diverse visualization outcomes and their critical engagement with the data and mapping processes. Based on our study findings we contribute insights into the design of free-form visualization tools for children, including the role of tool-based scaffolding mechanisms and shared interactions to guide visualization activities with children."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934281,JOUR,Critical Reflections on Visualization Authoring Systems,IEEE Transactions on Visualization and Computer Graphics,461,471,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934281,"['Satyanarayan, A.', 'Lee, B.', 'Ren, D.', 'Heer, J.', 'Stasko, J.', 'Thompson, J.', 'Brehmer, M.', 'Liu, Z.']","[""Data visualization"", ""Programming"", ""Visualization"", ""Authoring systems"", ""Grammar"", ""Interactive systems"", ""Libraries"", ""Critical reflection"", ""visualization authoring"", ""expressivity"", ""learnability"", ""reusability""]","An emerging generation of visualization authoring systems support expressive information visualization without textual programming. As they vary in their visualization models, system architectures, and user interfaces, it is challenging to directly compare these systems using traditional evaluative methods. Recognizing the value of contextualizing our decisions in the broader design space, we present critical reflections on three systems we developed --Lyra, Data Illustrator, and Charticulator. This paper surfaces knowledge that would have been daunting within the constituent papers of these three systems. We compare and contrast their (previously unmentioned) limitations and trade-offs between expressivity and learnability. We also reflect on common assumptions that we made during the development of our systems, thereby informing future research directions in visualization authoring systems."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934401,JOUR,Decoding a Complex Visualization in a Science Museum @ An Empirical Study,IEEE Transactions on Visualization and Computer Graphics,472,481,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934401,"['Ma, J.', 'Ma, K.', 'Frazier, J.']","[""Data visualization"", ""Decoding"", ""Sociology"", ""Statistics"", ""Visualization"", ""Oceans"", ""Lenses"", ""Museums"", ""informal science learning"", ""interactive exhibit"", ""public data visualization"", ""decoding"", ""visual encoding""]","This study describes a detailed analysis of museum visitors' decoding process as they used a visualization designed to support exploration of a large, complex dataset. Quantitative and qualitative analyses revealed that it took, on average, 43 seconds for visitors to decode enough of the visualization to see patterns and relationships in the underlying data represented, and 54 seconds to arrive at their first correct data interpretation. Furthermore, visitors decoded throughout and not only upon initial use of the visualization. The study analyzed think-aloud data to identify issues visitors had mapping the visual representations to their intended referents, examine why they occurred, and consider if and how these decoding issues were resolved. The paper also describes how multiple visual encodings both helped and hindered decoding and concludes with implications on the design and adaptation of visualizations for informal science learning venues."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934534,JOUR,Investigating Direct Manipulation of Graphical Encodings as a Method for User Interaction,IEEE Transactions on Visualization and Computer Graphics,482,491,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934534,"['Saket, B.', 'Huron, S.', 'Perin, C.', 'Endert, A.']","[""Bars"", ""Data visualization"", ""Encoding"", ""Visualization"", ""Image color analysis"", ""Tools"", ""Instruments"", ""Direct Manipulation"", ""Data Visualization""]","We investigate direct manipulation of graphical encodings as a method for interacting with visualizations. There is an increasing interest in developing visualization tools that enable users to perform operations by directly manipulating graphical encodings rather than external widgets such as checkboxes and sliders. Designers of such tools must decide which direct manipulation operations should be supported, and identify how each operation can be invoked. However, we lack empirical guidelines for how people convey their intended operations using direct manipulation of graphical encodings. We address this issue by conducting a qualitative study that examines how participants perform 15 operations using direct manipulation of standard graphical encodings. From this study, we 1) identify a list of strategies people employ to perform each operation, 2) observe commonalities in strategies across operations, and 3) derive implications to help designers leverage direct manipulation of graphical encoding as a method for user interaction."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934260,JOUR,Artifact-Based Rendering: Harnessing Natural and Traditional Visual Media for More Expressive and Engaging 3D Visualizations,IEEE Transactions on Visualization and Computer Graphics,492,502,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934260,"['Johnson, S.', 'Samsel, F.', 'Abram, G.', 'Olson, D.', 'Solis, A. J.', 'Herman, B.', 'Wolfram, P. J.', 'Lenglet, C.', 'Keefe, D. F.']","[""Data visualization"", ""Visualization"", ""Three-dimensional displays"", ""Rendering (computer graphics)"", ""Tools"", ""Media"", ""Image color analysis"", ""Visualization Design"", ""Art and Visualization"", ""Data Physicalization"", ""Multivariate Visualization""]","We introduce Artifact-Based Rendering (ABR), a framework of tools, algorithms, and processes that makes it possible to produce real, data-driven 3D scientific visualizations with a visual language derived entirely from colors, lines, textures, and forms created using traditional physical media or found in nature. A theory and process for ABR is presented to address three current needs: (i) designing better visualizations by making it possible for non-programmers to rapidly design and critique many alternative data-to-visual mappings; (ii) expanding the visual vocabulary used in scientific visualizations to depict increasingly complex multivariate data; (iii) bringing a more engaging, natural, and human-relatable handcrafted aesthetic to data visualization. New tools and algorithms to support ABR include front-end applets for constructing artifact-based colormaps, optimizing 3D scanned meshes for use in data visualization, and synthesizing textures from artifacts. These are complemented by an interactive rendering engine with custom algorithms and interfaces that demonstrate multiple new visual styles for depicting point, line, surface, and volume data. A within-the-research-team design study provides early evidence of the shift in visualization design processes that ABR is believed to enable when compared to traditional scientific visualization systems. Qualitative user feedback on applications to climate science and brain imaging support the utility of ABR for scientific discovery and public communication."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934282,JOUR,Designing for Mobile and Immersive Visual Analytics in the Field,IEEE Transactions on Visualization and Computer Graphics,503,513,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934282,"['Whitlock, M.', 'Wu, K.', 'Szafir, D. A.']","[""Data visualization"", ""Data collection"", ""Tools"", ""Decision making"", ""Mobile handsets"", ""Visual analytics"", ""Emergency services"", ""Immersive Analytics"", ""Augmented Reality"", ""Mobile Visualization"", ""Outdoor Visualization"", ""Emergency Response""]","Data collection and analysis in the field is critical for operations in domains such as environmental science and public safety. However, field workers currently face data- and platform-oriented issues in efficient data collection and analysis in the field, such as limited connectivity, screen space, and attentional resources. In this paper, we explore how visual analytics tools might transform field practices by more deeply integrating data into these operations. We use a design probe coupling mobile, cloud, and immersive analytics components to guide interviews with ten experts from five domains to explore how visual analytics could support data collection and analysis needs in the field. The results identify shortcomings of current approaches and target scenarios and design considerations for future field analysis systems. We embody these findings in FieldView, an extensible, open-source prototype designed to support critical use cases for situated field analysis. Our findings suggest the potential for integrating mobile and immersive technologies to enhance data's utility for various field operations and new directions for visual analytics tools to transform fieldwork."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934415,JOUR,Evaluating an Immersive Space-Time Cube Geovisualization for Intuitive Trajectory Data Exploration,IEEE Transactions on Visualization and Computer Graphics,514,524,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934415,"['Filho, J. A. W.', 'Stuerzlinger, W.', 'Nedel, L.']","[""Task analysis"", ""Trajectory"", ""Three-dimensional displays"", ""Data visualization"", ""Clutter"", ""Two dimensional displays"", ""Visualization"", ""Space-time cube"", ""Trajectory visualization"", ""Immersive analytics""]","A Space-Time Cube enables analysts to clearly observe spatio-temporal features in movement trajectory datasets in geovisualization. However, its general usability is impacted by a lack of depth cues, a reported steep learning curve, and the requirement for efficient 3D navigation. In this work, we investigate a Space-Time Cube in the Immersive Analytics domain. Based on a review of previous work and selecting an appropriate exploration metaphor, we built a prototype environment where the cube is coupled to a virtual representation of the analyst's real desk, and zooming and panning in space and time are intuitively controlled using mid-air gestures. We compared our immersive environment to a desktop-based implementation in a user study with 20 participants across 7 tasks of varying difficulty, which targeted different user interface features. To investigate how performance is affected in the presence of clutter, we explored two scenarios with different numbers of trajectories. While the quantitative performance was similar for the majority of tasks, large differences appear when we analyze the patterns of interaction and consider subjective metrics. The immersive version of the Space-Time Cube received higher usability scores, much higher user preference, and was rated to have a lower mental workload, without causing participants discomfort in 25-minute-long VR sessions."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934395,JOUR,The Impact of Immersion on Cluster Identification Tasks,IEEE Transactions on Visualization and Computer Graphics,525,535,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934395,"['Kraus, M.', 'Weiler, N.', 'Oelke, D.', 'Kehrer, J.', 'Keim, D. A.', 'Fuchs, J.']","[""Data visualization"", ""Three-dimensional displays"", ""Task analysis"", ""Two dimensional displays"", ""Visualization"", ""Virtual reality"", ""Dimensionality reduction"", ""Virtual reality"", ""evaluation"", ""visual analytics"", ""clustering""]","Recent developments in technology encourage the use of head-mounted displays (HMDs) as a medium to explore visualizations in virtual realities (VRs). VR environments (VREs) enable new, more immersive visualization design spaces compared to traditional computer screens. Previous studies in different domains, such as medicine, psychology, and geology, report a positive effect of immersion, e.g., on learning performance or phobia treatment effectiveness. Our work presented in this paper assesses the applicability of those findings to a common task from the information visualization (InfoVis) domain. We conducted a quantitative user study to investigate the impact of immersion on cluster identification tasks in scatterplot visualizations. The main experiment was carried out with 18 participants in a within-subjects setting using four different visualizations, (1) a 2D scatterplot matrix on a screen, (2) a 3D scatterplot on a screen, (3) a 3D scatterplot miniature in a VRE and (4) a fully immersive 3D scatterplot in a VRE. The four visualization design spaces vary in their level of immersion, as shown in a supplementary study. The results of our main study indicate that task performance differs between the investigated visualization design spaces in terms of accuracy, efficiency, memorability, sense of orientation, and user preference. In particular, the 2D visualization on the screen performed worse compared to the 3D visualizations with regard to the measured variables. The study shows that an increased level of immersion can be a substantial benefit in the context of 3D data and cluster detection."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934803,JOUR,"There Is No Spoon: Evaluating Performance, Space Use, and Presence with Expert Domain Users in Immersive Analytics",IEEE Transactions on Visualization and Computer Graphics,536,546,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934803,"['Batch, A.', 'Cunningham, A.', 'Cordeil, M.', 'Elmqvist, N.', 'Dwyer, T.', 'Thomas, B. H.', 'Marriott, K.']","[""Three-dimensional displays"", ""Data visualization"", ""Navigation"", ""Data analysis"", ""Macroeconomics"", ""Tools"", ""Design study"", ""evaluation"", ""economic analysis"", ""immersive analytics""]","Immersive analytics turns the very space surrounding the user into a canvas for data analysis, supporting human cognitive abilities in myriad ways. We present the results of a design study, contextual inquiry, and longitudinal evaluation involving professional economists using a Virtual Reality (VR) system for multidimensional visualization to explore actual economic data. Results from our preregistered evaluation highlight the varied use of space depending on context (exploration vs. presentation), the organization of space to support work, and the impact of immersion on navigation and orientation in the 3D analysis space."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934370,JOUR,Deadeye Visualization Revisited: Investigation of Preattentiveness and Applicability in Virtual Environments,IEEE Transactions on Visualization and Computer Graphics,547,557,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934370,"['Krekhov, A.', 'Cmentowski, S.', 'Waschk, A.', 'Kruger, J.']","[""Visualization"", ""Three-dimensional displays"", ""Image color analysis"", ""Rendering (computer graphics)"", ""Visual systems"", ""Color"", ""Stereo image processing"", ""Popout"", ""virtual reality"", ""preattentive vision"", ""volume rendering"", ""dichoptic presentation"", ""binocular rivalry""]","Visualizations rely on highlighting to attract and guide our attention. To make an object of interest stand out independently from a number of distractors, the underlying visual cue, e.g., color, has to be preattentive. In our prior work, we introduced Deadeye as an instantly recognizable highlighting technique that works by rendering the target object for one eye only. In contrast to prior approaches, Deadeye excels by not modifying any visual properties of the target. However, in the case of 2D visualizations, the method requires an additional setup to allow dichoptic presentation, which is a considerable drawback. As a follow-up to requests from the community, this paper explores Deadeye as a highlighting technique for 3D visualizations, because such stereoscopic scenarios support dichoptic presentation out of the box. Deadeye suppresses binocular disparities for the target object, so we cannot assume the applicability of our technique as a given fact. With this motivation, the paper presents quantitative evaluations of Deadeye in VR, including configurations with multiple heterogeneous distractors as an important robustness challenge. After confirming the preserved preattentiveness (all average accuracies above 90%) under such real-world conditions, we explore VR volume rendering as an example application scenario for Deadeye. We depict a possible workflow for integrating our technique, conduct an exploratory survey to demonstrate benefits and limitations, and finally provide related design implications."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934614,JOUR,Interactive Learning for Identifying Relevant Tweets to Support Real-time Situational Awareness,IEEE Transactions on Visualization and Computer Graphics,558,568,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934614,"['Snyder, L. S.', 'Lin, Y.', 'Karimzadeh, M.', 'Goldwasser, D.', 'Ebert, D. S.']","[""Real-time systems"", ""Social networking (online)"", ""Computational modeling"", ""Visual analytics"", ""Machine learning"", ""Adaptation models"", ""Training"", ""Interactive machine learning"", ""human-computer interaction"", ""social media analytics"", ""emergency/disaster management"", ""situational awareness""]","Various domain users are increasingly leveraging real-time social media data to gain rapid situational awareness. However, due to the high noise in the deluge of data, effectively determining semantically relevant information can be difficult, further complicated by the changing definition of relevancy by each end user for different events. The majority of existing methods for short text relevance classification fail to incorporate users' knowledge into the classification process. Existing methods that incorporate interactive user feedback focus on historical datasets. Therefore, classifiers cannot be interactively retrained for specific events or user-dependent needs in real-time. This limits real-time situational awareness, as streaming data that is incorrectly classified cannot be corrected immediately, permitting the possibility for important incoming data to be incorrectly classified as well. We present a novel interactive learning framework to improve the classification process in which the user iteratively corrects the relevancy of tweets in real-time to train the classification model on-the-fly for immediate predictive improvements. We computationally evaluate our classification model adapted to learn at interactive rates. Our results show that our approach outperforms state-of-the-art machine learning models. In addition, we integrate our framework with the extended Social Media Analytics and Reporting Toolkit (SMART) 2.0 system, allowing the use of our interactive learning framework within a visual analytics system tailored for real-time situational awareness. To demonstrate our framework's effectiveness, we provide domain expert feedback from first responders who used the extended SMART 2.0 system."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934658,JOUR,"LightGuider: Guiding Interactive Lighting Design using Suggestions, Provenance, and Quality Visualization",IEEE Transactions on Visualization and Computer Graphics,569,578,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934658,"['Walch, A.', 'Schwarzler, M.', 'Luksch, C.', 'Eisemann, E.', 'Gschwandtner, T.']","[""Lighting"", ""Visualization"", ""Solid modeling"", ""Data visualization"", ""Three-dimensional displays"", ""History"", ""Computational modeling"", ""guidance"", ""3D modeling"", ""lighting design"", ""provenance"", ""global illumination""]","LightGuider is a novel guidance-based approach to interactive lighting design, which typically consists of interleaved 3D modeling operations and light transport simulations. Rather than having designers use a trial-and-error approach to match their illumination constraints and aesthetic goals, LightGuider supports the process by simulating potential next modeling steps that can deliver the most significant improvements. LightGuider takes predefined quality criteria and the current focus of the designer into account to visualize suggestions for lighting-design improvements via a specialized provenance tree. This provenance tree integrates snapshot visualizations of how well a design meets the given quality criteria weighted by the designer's preferences. This integration facilitates the analysis of quality improvements over the course of a modeling workflow as well as the comparison of alternative design solutions. We evaluate our approach with three lighting designers to illustrate its usefulness."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934275,JOUR,PlanningVis: A Visual Analytics Approach to Production Planning in Smart Factories,IEEE Transactions on Visualization and Computer Graphics,579,589,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934275,"['Sun, D.', 'Huang, R.', 'Chen, Y.', 'Wang, Y.', 'Zeng, J.', 'Yuan, M.', 'Pong, T.', 'Qu, H.']","[""Production facilities"", ""Task analysis"", ""Optimization"", ""Manufacturing"", ""Raw materials"", ""Production Planning"", ""Time Series Data"", ""Comparative Analysis"", ""Visual Analytics"", ""Smart Factory"", ""Industry 4.0""]","Production planning in the manufacturing industry is crucial for fully utilizing factory resources (e.g., machines, raw materials and workers) and reducing costs. With the advent of industry 4.0, plenty of data recording the status of factory resources have been collected and further involved in production planning, which brings an unprecedented opportunity to understand, evaluate and adjust complex production plans through a data-driven approach. However, developing a systematic analytics approach for production planning is challenging due to the large volume of production data, the complex dependency between products, and unexpected changes in the market and the plant. Previous studies only provide summarized results and fail to show details for comparative analysis of production plans. Besides, the rapid adjustment to the plan in the case of an unanticipated incident is also not supported. In this paper, we propose PlanningVis, a visual analytics system to support the exploration and comparison of production plans with three levels of details: a plan overview presenting the overall difference between plans, a product view visualizing various properties of individual products, and a production detail view displaying the product dependency and the daily production details in related factories. By integrating an automatic planning algorithm with interactive visual explorations, PlanningVis can facilitate the efficient optimization of daily production planning as well as support a quick response to unanticipated incidents in manufacturing. Two case studies with real-world data and carefully designed interviews with domain experts demonstrate the effectiveness and usability of PlanningVis."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934655,JOUR,Visual Analytics for Electromagnetic Situation Awareness in Radio Monitoring and Management,IEEE Transactions on Visualization and Computer Graphics,590,600,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934655,"['Zhao, Y.', 'Luo, X.', 'Lin, X.', 'Wang, H.', 'Kui, X.', 'Zhou, F.', 'Wang, J.', 'Chen, Y.', 'Chen, W.']","[""Data visualization"", ""Electromagnetics"", ""Time-frequency analysis"", ""Monitoring"", ""Computational modeling"", ""Visual analytics"", ""Radio monitoring and management"", ""radio signal data"", ""radio spectrum data"", ""situation awareness"", ""visual analytics""]","Traditional radio monitoring and management largely depend on radio spectrum data analysis, which requires considerable domain experience and heavy cognition effort and frequently results in incorrect signal judgment and incomprehensive situation awareness. Faced with increasingly complicated electromagnetic environments, radio supervisors urgently need additional data sources and advanced analytical technologies to enhance their situation awareness ability. This paper introduces a visual analytics approach for electromagnetic situation awareness. Guided by a detailed scenario and requirement analysis, we first propose a signal clustering method to process radio signal data and a situation assessment model to obtain qualitative and quantitative descriptions of the electromagnetic situations. We then design a two-module interface with a set of visualization views and interactions to help radio supervisors perceive and understand the electromagnetic situations by a joint analysis of radio signal data and radio spectrum data. Evaluations on real-world data sets and an interview with actual users demonstrate the effectiveness of our prototype system. Finally, we discuss the limitations of the proposed approach and provide future work directions."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934660,JOUR,sPortfolio: Stratified Visual Analysis of Stock Portfolios,IEEE Transactions on Visualization and Computer Graphics,601,610,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934660,"['Yue, X.', 'Bai, J.', 'Liu, Q.', 'Tang, Y.', 'Puri, A.', 'Li, K.', 'Qu, H.']","[""Portfolios"", ""Data visualization"", ""Investment"", ""Industries"", ""Time series analysis"", ""Visual analytics"", ""Stock portfolio"", ""visual analytics"", ""factor investment"", ""financial data analysis""]","Quantitative Investment, built on the solid foundation of robust financial theories, is at the center stage in investment industry today. The essence of quantitative investment is the multi-factor model, which explains the relationship between the risk and return of equities. However, the multi-factor model generates enormous quantities of factor data, through which even experienced portfolio managers find it difficult to navigate. This has led to portfolio analysis and factor research being limited by a lack of intuitive visual analytics tools. Previous portfolio visualization systems have mainly focused on the relationship between the portfolio return and stock holdings, which is insufficient for making actionable insights or understanding market trends. In this paper, we present s Portfolio, which, to the best of our knowledge, is the first visualization that attempts to explore the factor investment area. In particular, sPortfolio provides a holistic overview of the factor data and aims to facilitate the analysis at three different levels: a Risk-Factor level, for a general market situation analysis; a Multiple-Portfolio level, for understanding the portfolio strategies; and a Single-Portfolio level, for investigating detailed operations. The system's effectiveness and usability are demonstrated through three case studies. The system has passed its pilot study and is soon to be deployed in industry."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934555,JOUR,Pattern-Driven Navigation in 2D Multiscale Visualizations with Scalable Insets,IEEE Transactions on Visualization and Computer Graphics,611,621,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934555,"['Lekschas, F.', 'Behrisch, M.', 'Bach, B.', 'Kerpedjiev, P.', 'Gehlenborg, N.', 'Pfister, H.']","[""Visualization"", ""Navigation"", ""Genomics"", ""Bioinformatics"", ""Data visualization"", ""Labeling"", ""Lenses"", ""Guided Navigation"", ""Pattern Exploration"", ""Multiscale Visualizations"", ""Gigapixel Images"", ""Geospatial Maps"", ""Genomics""]","We present Scalable Insets, a technique for interactively exploring and navigating large numbers of annotated patterns in multiscale visualizations such as gigapixel images, matrices, or maps. Exploration of many but sparsely-distributed patterns in multiscale visualizations is challenging as visual representations change across zoom levels, context and navigational cues get lost upon zooming, and navigation is time consuming. Our technique visualizes annotated patterns too small to be identifiable at certain zoom levels using insets, i.e., magnified thumbnail views of the annotated patterns. Insets support users in searching, comparing, and contextualizing patterns while reducing the amount of navigation needed. They are dynamically placed either within the viewport or along the boundary of the viewport to offer a compromise between locality and context preservation. Annotated patterns are interactively clustered by location and type. They are visually represented as an aggregated inset to provide scalable exploration within a single viewport. In a controlled user study with 18 participants, we found that Scalable Insets can speed up visual search and improve the accuracy of pattern comparison at the cost of slower frequency estimation compared to a baseline technique. A second study with 6 experts in the field of genomics showed that Scalable Insets is easy to learn and provides first insights into how Scalable Insets can be applied in an open-ended data exploration scenario."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934612,JOUR,Multi-Scale Procedural Animations of Microtubule Dynamics Based on Measured Data,IEEE Transactions on Visualization and Computer Graphics,622,632,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934612,"['Klein, T.', 'Viola, I.', 'Groller, E.', 'Mindek, P.']","[""Animation"", ""Biological system modeling"", ""Data visualization"", ""Computational modeling"", ""Visualization"", ""Data models"", ""Procedural modeling"", ""molecular visualization"", ""animation"", ""microtubules""]","Biologists often use computer graphics to visualize structures, which due to physical limitations are not possible to image with a microscope. One example for such structures are microtubules, which are present in every eukaryotic cell. They are part of the cytoskeleton maintaining the shape of the cell and playing a key role in the cell division. In this paper, we propose a scientifically-accurate multi-scale procedural model of microtubule dynamics as a novel application scenario for procedural animation, which can generate visualizations of their overall shape, molecular structure, as well as animations of the dynamic behaviour of their growth and disassembly. The model is spanning from tens of micrometers down to atomic resolution. All the aspects of the model are driven by scientific data. The advantage over a traditional, manual animation approach is that when the underlying data change, for instance due to new evidence, the model can be recreated immediately. The procedural animation concept is presented in its generic form, with several novel extensions, facilitating an easy translation to other domains with emergent multi-scale behavior."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934259,JOUR,OpenSpace: A System for Astrographics,IEEE Transactions on Visualization and Computer Graphics,633,642,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934259,"['Bock, A.', 'Axelsson, E.', 'Costa, J.', 'Payne, G.', 'Acinapura, M.', 'Trakinski, V.', 'Emmart, C.', 'Silva, C.', 'Hansen, C.', 'Ynnerman, A.']","[""Data visualization"", ""Tools"", ""Rendering (computer graphics)"", ""Data models"", ""Astronomy"", ""Space vehicles"", ""Space missions"", ""Astrographics"", ""astronomy"", ""astrophysics"", ""system""]","Human knowledge about the cosmos is rapidly increasing as instruments and simulations are generating new data supporting the formation of theory and understanding of the vastness and complexity of the universe. OpenSpace is a software system that takes on the mission of providing an integrated view of all these sources of data and supports interactive exploration of the known universe from the millimeter scale showing instruments on spacecrafts to billions of light years when visualizing the early universe. The ambition is to support research in astronomy and space exploration, science communication at museums and in planetariums as well as bringing exploratory astrographics to the class room. There is a multitude of challenges that need to be met in reaching this goal such as the data variety, multiple spatio-temporal scales, collaboration capabilities, etc. Furthermore, the system has to be flexible and modular to enable rapid prototyping and inclusion of new research results or space mission data and thereby shorten the time from discovery to dissemination. To support the different use cases the system has to be hardware agnostic and support a range of platforms and interaction paradigms. In this paper we describe how OpenSpace meets these challenges in an open source effort that is paving the path for the next generation of interactive astrographics."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934258,JOUR,Scale-Space Splatting: Reforming Spacetime for Cross-Scale Exploration of Integral Measures in Molecular Dynamics,IEEE Transactions on Visualization and Computer Graphics,643,653,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934258,"['Palenik, J.', 'Byska, J.', 'Bruckner, S.', 'Hauser, H.']","[""Data visualization"", ""Computational modeling"", ""Time series analysis"", ""Atmospheric measurements"", ""Particle measurements"", ""Analytical models"", ""Kernel"", ""Scale space"", ""time-series"", ""scientific simulation"", ""multi-scale analysis"", ""space-time cube"", ""molecular dynamics""]","Understanding large amounts of spatiotemporal data from particle-based simulations, such as molecular dynamics, often relies on the computation and analysis of aggregate measures. These, however, by virtue of aggregation, hide structural information about the space/time localization of the studied phenomena. This leads to degenerate cases where the measures fail to capture distinct behaviour. In order to drill into these aggregate values, we propose a multi-scale visual exploration technique. Our novel representation, based on partial domain aggregation, enables the construction of a continuous scale-space for discrete datasets and the simultaneous exploration of scales in both space and time. We link these two scale-spaces in a scale-space space-time cube and model linked views as orthogonal slices through this cube, thus enabling the rapid identification of spatio-temporal patterns at multiple scales. To demonstrate the effectiveness of our approach, we showcase an advanced exploration of a protein-ligand simulation."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934334,JOUR,Scale Trotter: Illustrative Visual Travels Across Negative Scales,IEEE Transactions on Visualization and Computer Graphics,654,664,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934334,"['Halladjian, S.', 'Miao, H.', 'Kou@il, D.', 'Groller, M. E.', 'Viola, I.', 'Isenberg, T.']","[""Visualization"", ""Data visualization"", ""Genomics"", ""Bioinformatics"", ""DNA"", ""Biological cells"", ""Three-dimensional displays"", ""Multi-scale visualization"", ""scale transition"", ""abstraction"", ""human genome"", ""DNA"", ""Hi-C data""]","We present ScaleTrotter, a conceptual framework for an interactive, multi-scale visualization of biological mesoscale data and, specifically, genome data. ScaleTrotter allows viewers to smoothly transition from the nucleus of a cell to the atomistic composition of the DNA, while bridging several orders of magnitude in scale. The challenges in creating an interactive visualization of genome data are fundamentally different in several ways from those in other domains like astronomy that require a multi-scale representation as well. First, genome data has intertwined scale levels--the DNA is an extremely long, connected molecule that manifests itself at all scale levels. Second, elements of the DNA do not disappear as one zooms out--instead the scale levels at which they are observed group these elements differently. Third, we have detailed information and thus geometry for the entire dataset and for all scale levels, posing a challenge for interactive visual exploration. Finally, the conceptual scale levels for genome data are close in scale space, requiring us to find ways to visually embed a smaller scale into a coarser one. We address these challenges by creating a new multi-scale visualization concept. We use a scale-dependent camera model that controls the visual embedding of the scales into their respective parents, the rendering of a subset of the scale hierarchy, and the location, size, and scope of the view. In traversing the scales, ScaleTrotter is roaming between 2D and 3D visual representations that are depicted in integrated visuals. We discuss, specifically, how this form of multi-scale visualization follows from the specific characteristics of the genome data and describe its implementation. Finally, we discuss the implications of our work to the general illustrative depiction of multi-scale data."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934396,JOUR,A Deep Generative Model for Graph Layout,IEEE Transactions on Visualization and Computer Graphics,665,675,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934396,"['Kwon, O.', 'Ma, K.']","[""Layout"", ""Training"", ""Visualization"", ""Task analysis"", ""Data visualization"", ""Machine learning"", ""Data models"", ""Graph"", ""network"", ""visualization"", ""layout"", ""machine learning"", ""deep learning"", ""neural network"", ""generative model"", ""autoencoder""]","Different layouts can characterize different aspects of the same graph. Finding a ""good@ layout of a graph is thus an important task for graph visualization. In practice, users often visualize a graph in multiple layouts by using different methods and varying parameter settings until they find a layout that best suits the purpose of the visualization. However, this trial-and-error process is often haphazard and time-consuming. To provide users with an intuitive way to navigate the layout design space, we present a technique to systematically visualize a graph in diverse layouts using deep generative models. We design an encoder-decoder architecture to learn a model from a collection of example layouts, where the encoder represents training examples in a latent space and the decoder produces layouts from the latent space. In particular, we train the model to construct a two-dimensional latent space for users to easily explore and generate various layouts. We demonstrate our approach through quantitative and qualitative evaluations of the generated layouts. The results of our evaluations show that our model is capable of learning and generalizing abstract concepts of graph layouts, not just memorizing the training examples. In summary, this paper presents a fundamentally new approach to graph visualization where a machine learning model learns to visualize a graph from examples without manually-defined heuristics."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934798,JOUR,DeepDrawing: A Deep Learning Approach to Graph Drawing,IEEE Transactions on Visualization and Computer Graphics,676,686,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934798,"['Wang, Y.', 'Jin, Z.', 'Wang, Q.', 'Cui, W.', 'Ma, T.', 'Qu, H.']","[""Deep learning"", ""Layout"", ""Neural networks"", ""Training"", ""Convolution"", ""Data models"", ""Visualization"", ""Graph Drawing"", ""Deep Learning"", ""LSTM"", ""Procrustes Analysis""]","Node-link diagrams are widely used to facilitate network explorations. However, when using a graph drawing technique to visualize networks, users often need to tune different algorithm-specific parameters iteratively by comparing the corresponding drawing results in order to achieve a desired visual effect. This trial and error process is often tedious and time-consuming, especially for non-expert users. Inspired by the powerful data modelling and prediction capabilities of deep learning techniques, we explore the possibility of applying deep learning techniques to graph drawing. Specifically, we propose using a graph-LSTM-based approach to directly map network structures to graph drawings. Given a set of layout examples as the training dataset, we train the proposed graph-LSTM-based model to capture their layout characteristics. Then, the trained model is used to generate graph drawings in a similar style for new networks. We evaluated the proposed approach on two special types of layouts (i.e., grid layouts and star layouts) and two general types of layouts (i.e., ForceAtlas2 and PivotMDS) in both qualitative and quantitative ways. The results provide support for the effectiveness of our approach. We also conducted a time cost assessment on the drawings of small graphs with 20 to 50 nodes. We further report the lessons we learned and discuss the limitations and future work."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934805,JOUR,Interactive Structure-aware Blending of Diverse Edge Bundling Visualizations,IEEE Transactions on Visualization and Computer Graphics,687,696,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934805,"['Wang, Y.', 'Xue, M.', 'Wang, Y.', 'Yan, X.', 'Chen, B.', 'Fu, C.', 'Hurter, C.']","[""Layout"", ""Data visualization"", ""Visualization"", ""Task analysis"", ""Strain"", ""Smoothing methods"", ""Optimization"", ""path visualization"", ""trajectory visualization"", ""edge bundles""]","Many edge bundling techniques (i.e., data simplification as a support for data visualization and decision making) exist but they are not directly applicable to any kind of dataset and their parameters are often too abstract and difficult to set up. As a result, this hinders the user ability to create efficient aggregated visualizations. To address these issues, we investigated a novel way of handling visual aggregation with a task-driven and user-centered approach. Given a graph, our approach produces a decluttered view as follows: first, the user investigates different edge bundling results and specifies areas, where certain edge bundling techniques would provide user-desired results. Second, our system then computes a smooth and structural preserving transition between these specified areas. Lastly, the user can further fine-tune the global visualization with a direct manipulation technique to remove the local ambiguity and to apply different visual deformations. In this paper, we provide details for our design rationale and implementation. Also, we show how our algorithm gives more suitable results compared to current edge bundling techniques, and in the end, we provide concrete instances of usages, where the algorithm combines various edge bundling results to support diverse data exploration and visualizations."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934802,JOUR,Persistent Homology Guided Force-Directed Graph Layouts,IEEE Transactions on Visualization and Computer Graphics,697,707,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934802,"['Suh, A.', 'Hajij, M.', 'Wang, B.', 'Scheidegger, C.', 'Rosen, P.']","[""Layout"", ""Feature extraction"", ""Visualization"", ""Extraterrestrial measurements"", ""Clutter"", ""Data visualization"", ""Graph drawing"", ""force-directed layout"", ""Topological Data Analysis"", ""persistent homology""]","Graphs are commonly used to encode relationships among entities, yet their abstractness makes them difficult to analyze. Node-link diagrams are popular for drawing graphs, and force-directed layouts provide a flexible method for node arrangements that use local relationships in an attempt to reveal the global shape of the graph. However, clutter and overlap of unrelated structures can lead to confusing graph visualizations. This paper leverages the persistent homology features of an undirected graph as derived information for interactive manipulation of force-directed layouts. We first discuss how to efficiently extract 0-dimensional persistent homology features from both weighted and unweighted undirected graphs. We then introduce the interactive persistence barcode used to manipulate the force-directed graph layout. In particular, the user adds and removes contracting and repulsing forces generated by the persistent homology features, eventually selecting the set of persistent homology features that most improve the layout. Finally, we demonstrate the utility of our approach across a variety of synthetic and real datasets."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934313,JOUR,Accelerated Monte Carlo Rendering of Finite-Time Lyapunov Exponents,IEEE Transactions on Visualization and Computer Graphics,708,718,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934313,"['Rojo, I. B.', 'Gross, M.', 'Gunther, T.']","[""Rendering (computer graphics)"", ""Acceleration"", ""Monte Carlo methods"", ""Scattering"", ""Feature extraction"", ""Data visualization"", ""Animation"", ""Scientific visualization"", ""Monte Carlo"", ""feature extraction"", ""finite-time Lyapunov exponents"", ""gradient domain"", ""Fourier""]","Time-dependent fluid flows often contain numerous hyperbolic Lagrangian coherent structures, which act as transport barriers that guide the advection. The finite-time Lyapunov exponent is a commonly-used approximation to locate these repelling or attracting structures. Especially on large numerical simulations, the FTLE ridges can become arbitrarily sharp and very complex. Thus, the discrete sampling onto a grid for a subsequent direct volume rendering is likely to miss sharp ridges in the visualization. For this reason, an unbiased Monte Carlo-based rendering approach was recently proposed that treats the FTLE field as participating medium with single scattering. This method constructs a ground truth rendering without discretization, but it is prohibitively slow with render times in the order of days or weeks for a single image. In this paper, we accelerate the rendering process significantly, which allows us to compute video sequence of high-resolution FTLE animations in a much more reasonable time frame. For this, we follow two orthogonal approaches to improve on the rendering process: the volumetric light path integration in gradient domain and an acceleration of the transmittance estimation. We analyze the convergence and performance of the proposed method and demonstrate the approach by rendering complex FTLE fields in several 3D vector fields."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934367,JOUR,Analysis of the Near-Wall Flow in a Turbine Cascade by Splat Visualization,IEEE Transactions on Visualization and Computer Graphics,719,728,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934367,"['Nsonga, B.', 'Scheuermann, G.', 'Gumhold, S.', 'Ventosa-Molina, J.', 'Koschichow, D.', 'Frohlich, J.']","[""Blades"", ""Turbines"", ""Heat transfer"", ""Numerical models"", ""Geometry"", ""Temperature measurement"", ""Flow Visualization"", ""Visualization in Physical Sciences and Engineering"", ""Feature Detection and Tracking"", ""Vector Field Data""]","Turbines are essential components of jet planes and power plants. Therefore, their efficiency and service life are of central engineering interest. In the case of jet planes or thermal power plants, the heating of the turbines due to the hot gas flow is critical. Besides effective cooling, it is a major goal of engineers to minimize heat transfer between gas flow and turbine by design. Since it is known that splat events have a substantial impact on the heat transfer between flow and immersed surfaces, we adapt a splat detection and visualization method to a turbine cascade simulation in this case study. Because splat events are small phenomena, we use a direct numerical simulation resolving the turbulence in the flow as the base of our analysis. The outcome shows promising insights into splat formation and its relation to vortex structures. This may lead to better turbine design in the future."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934541,JOUR,A Recursive Subdivision Technique for Sampling Multi-class Scatterplots,IEEE Transactions on Visualization and Computer Graphics,729,738,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934541,"['Chen, X.', 'Ge, T.', 'Zhang, J.', 'Chen, B.', 'Fu, C.', 'Deussen, O.', 'Wang, Y.']","[""Visualization"", ""Data visualization"", ""Measurement"", ""Sampling methods"", ""Estimation"", ""Clutter"", ""Image color analysis"", ""Scatterplot"", ""multi-class sampling"", ""kd-tree"", ""outlier"", ""relative density""]","We present a non-uniform recursive sampling technique for multi-class scatterplots, with the specific goal of faithfully presenting relative data and class densities, while preserving major outliers in the plots. Our technique is based on a customized binary kd-tree, in which leaf nodes are created by recursively subdividing the underlying multi-class density map. By backtracking, we merge leaf nodes until they encompass points of all classes for our subsequently applied outlier-aware multi-class sampling strategy. A quantitative evaluation shows that our approach can better preserve outliers and at the same time relative densities in multi-class scatterplots compared to the previous approaches, several case studies demonstrate the effectiveness of our approach in exploring complex and real world data."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934799,JOUR,Data Sampling in Multi-view and Multi-class Scatterplots via Set Cover Optimization,IEEE Transactions on Visualization and Computer Graphics,739,748,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934799,"['Hu, R.', 'Sha, T.', 'Kaick, O. Van', 'Deussen, O.', 'Huang, H.']","[""Sampling methods"", ""Visualization"", ""Optimization"", ""Kernel"", ""Two dimensional displays"", ""Image color analysis"", ""Measurement"", ""Sampling"", ""Scatterplot"", ""SPLOM"", ""Exact Cover Problem""]","We present a method for data sampling in scatterplots by jointly optimizing point selection for different views or classes. Our method uses space-filling curves (Z-order curves) that partition a point set into subsets that, when covered each by one sample, provide a sampling or coreset with good approximation guarantees in relation to the original point set. For scatterplot matrices with multiple views, different views provide different space-filling curves, leading to different partitions of the given point set. For multi-class scatterplots, the focus on either per-class distribution or global distribution provides two different partitions of the given point set that need to be considered in the selection of the coreset. For both cases, we convert the coreset selection problem into an Exact Cover Problem (ECP), and demonstrate with quantitative and qualitative evaluations that an approximate solution that solves the ECP efficiently is able to provide high-quality samplings."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934432,JOUR,Discriminability Tests for Visualization Effectiveness and Scalability,IEEE Transactions on Visualization and Computer Graphics,749,758,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934432,"['Veras, R.', 'Collins, C.']","[""Data visualization"", ""Visualization"", ""Encoding"", ""Scalability"", ""Image coding"", ""Task analysis"", ""Indexes"", ""Scalability"", ""Discriminability"", ""Simulation"", ""Perception""]","The scalability of a particular visualization approach is limited by the ability for people to discern differences between plots made with different datasets. Ideally, when the data changes, the visualization changes in perceptible ways. This relation breaks down when there is a mismatch between the encoding and the character of the dataset being viewed. Unfortunately, visualizations are often designed and evaluated without fully exploring how they will respond to a wide variety of datasets. We explore the use of an image similarity measure, the Multi-Scale Structural Similarity Index (MS-SSIM), for testing the discriminability of a data visualization across a variety of datasets. MS-SSIM is able to capture the similarity of two visualizations across multiple scales, including low level granular changes and high level patterns. Significant data changes that are not captured by the MS-SSIM indicate visualizations of low discriminability and effectiveness. The measure's utility is demonstrated with two empirical studies. In the first, we compare human similarity judgments and MS-SSIM scores for a collection of scatterplots. In the second, we compute the discriminability values for a set of basic visualizations and compare them with empirical measurements of effectiveness. In both cases, the analyses show that the computational measure is able to approximate empirical results. Our approach can be used to rank competing encodings on their discriminability and to aid in selecting visualizations for a particular type of data distribution."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934796,JOUR,Improving the Robustness of Scagnostics,IEEE Transactions on Visualization and Computer Graphics,759,769,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934796,"['Wang, Y.', 'Wang, Z.', 'Liu, T.', 'Correll, M.', 'Cheng, Z.', 'Deussen, O.', 'Sedlmair, M.']","[""Visualization"", ""Atmospheric measurements"", ""Particle measurements"", ""Sensitivity"", ""Density measurement"", ""Robustness"", ""Perturbation methods"", ""Scagnostics"", ""scatterplots"", ""sensitivity analysis"", ""Robust Scagnostics""]","In this paper, we examine the robustness of scagnostics through a series of theoretical and empirical studies. First, we investigate the sensitivity of scagnostics by employing perturbing operations on more than 60M synthetic and real-world scatterplots. We found that two scagnostic measures, Outlying and Clumpy, are overly sensitive to data binning. To understand how these measures align with human judgments of visual features, we conducted a study with 24 participants, which reveals that i) humans are not sensitive to small perturbations of the data that cause large changes in both measures, and ii) the perception of clumpiness heavily depends on per-cluster topologies and structures. Motivated by these results, we propose Robust Scagnostics (RScag) by combining adaptive binning with a hierarchy-based form of scagnostics. An analysis shows that RScag improves on the robustness of original scagnostics, aligns better with human judgments, and is equally fast as the traditional scagnostic measures."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934811,JOUR,Winglets: Visualizing Association with Uncertainty in Multi-class Scatterplots,IEEE Transactions on Visualization and Computer Graphics,770,779,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934811,"['Lu, M.', 'Wang, S.', 'Lanir, J.', 'Fish, N.', 'Yue, Y.', 'Cohen-Or, D.', 'Huang, H.']","[""Visualization"", ""Uncertainty"", ""Image color analysis"", ""Shape"", ""Clustering algorithms"", ""Encoding"", ""Scatterplot"", ""Gestalt laws"", ""Association"", ""Uncertainty""]","This work proposes Winglets, an enhancement to the classic scatterplot to better perceptually pronounce multiple classes by improving the perception of association and uncertainty of points to their related cluster. Designed as a pair of dual-sided strokes belonging to a data point, Winglets leverage the Gestalt principle of Closure to shape the perception of the form of the clusters, rather than use an explicit divisive encoding. Through a subtle design of two dominant attributes, length and orientation, Winglets enable viewers to perform a mental completion of the clusters. A controlled user study was conducted to examine the efficiency of Winglets in perceiving the cluster association and the uncertainty of certain points. The results show Winglets form a more prominent association of points into clusters and improve the perception of associating uncertainty."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934335,JOUR,Void-and-Cluster Sampling of Large Scattered Data and Trajectories,IEEE Transactions on Visualization and Computer Graphics,780,789,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934335,"['Rapp, T.', 'Peters, C.', 'Dachsbacher, C.']","[""Data visualization"", ""Trajectory"", ""Measurement uncertainty"", ""Data models"", ""Loading"", ""Computational modeling"", ""Entropy"", ""Data reduction"", ""sampling"", ""blue noise"", ""entropy-based sampling"", ""scattered data"", ""pathlines""]","We propose a data reduction technique for scattered data based on statistical sampling. Our void-and-cluster sampling technique finds a representative subset that is optimally distributed in the spatial domain with respect to the blue noise property. In addition, it can adapt to a given density function, which we use to sample regions of high complexity in the multivariate value domain more densely. Moreover, our sampling technique implicitly defines an ordering on the samples that enables progressive data loading and a continuous level-of-detail representation. We extend our technique to sample time-dependent trajectories, for example pathlines in a time interval, using an efficient and iterative approach. Furthermore, we introduce a local and continuous error measure to quantify how well a set of samples represents the original dataset. We apply this error measure during sampling to guide the number of samples that are taken. Finally, we use this error measure and other quantities to evaluate the quality, performance, and scalability of our algorithm."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934434,JOUR,SmartCube: An Adaptive Data Management Architecture for the Real-Time Visualization of Spatiotemporal Datasets,IEEE Transactions on Visualization and Computer Graphics,790,799,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934434,"['Liu, C.', 'Wu, C.', 'Shao, H.', 'Yuan, X.']","[""Data visualization"", ""Data structures"", ""Spatiotemporal phenomena"", ""Memory management"", ""Task analysis"", ""Real-time systems"", ""Data aggregation"", ""data management"", ""spatial-temporal data""]","Interactive visualization and exploration of large spatiotemporal data sets is difficult without carefully-designed data pre-processing and management tools. We propose a novel architecture for spatiotemporal data management. The architecture can dynamically update itself based on user queries. Datasets is stored in a tree-like structure to support memory sharing among cuboids in a logical structure of data cubes. An update mechanism is designed to create or remove cuboids on it, according to the analysis of the user queries, with the consideration of memory size limitation. Data structure is dynamically optimized according to different user queries. During a query process, user queries are recorded to predict the performance increment of the new cuboid. The creation or deletion of a cuboid is determined by performance increment. Experiment results show that our prototype system deliveries good performance towards user queries on different spatiotemporal datasets, which costing small memory size with comparable performance compared with other state-of-the-art algorithms."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934670,JOUR,AirVis: Visual Analytics of Air Pollution Propagation,IEEE Transactions on Visualization and Computer Graphics,800,810,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934670,"['Deng, Z.', 'Weng, D.', 'Chen, J.', 'Liu, R.', 'Wang, Z.', 'Bao, J.', 'Zheng, Y.', 'Wu, Y.']","[""Air pollution"", ""Data visualization"", ""Data mining"", ""Atmospheric modeling"", ""Transportation"", ""Spatiotemporal phenomena"", ""Air pollution propagation"", ""pattern mining"", ""graph visualization""]","Air pollution has become a serious public health problem for many cities around the world. To find the causes of air pollution, the propagation processes of air pollutants must be studied at a large spatial scale. However, the complex and dynamic wind fields lead to highly uncertain pollutant transportation. The state-of-the-art data mining approaches cannot fully support the extensive analysis of such uncertain spatiotemporal propagation processes across multiple districts without the integration of domain knowledge. The limitation of these automated approaches motivates us to design and develop AirVis, a novel visual analytics system that assists domain experts in efficiently capturing and interpreting the uncertain propagation patterns of air pollution based on graph visualizations. Designing such a system poses three challenges: a) the extraction of propagation patterns; b) the scalability of pattern presentations; and c) the analysis of propagation processes. To address these challenges, we develop a novel pattern mining framework to model pollutant transportation and extract frequent propagation patterns efficiently from large-scale atmospheric data. Furthermore, we organize the extracted patterns hierarchically based on the minimum description length (MDL) principle and empower expert users to explore and analyze these patterns effectively on the basis of pattern topologies. We demonstrated the effectiveness of our approach through two case studies conducted with a real-world dataset and positive feedback from domain experts."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934657,JOUR,OD Morphing: Balancing Simplicity with Faithfulness for OD Bundling,IEEE Transactions on Visualization and Computer Graphics,811,821,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934657,"['Lyu, Y.', 'Liu, X.', 'Chen, H.', 'Mangal, A.', 'Liu, K.', 'Chen, C.', 'Lim, B.']","[""Trajectory"", ""Data visualization"", ""Roads"", ""Public transportation"", ""Visualization"", ""Image edge detection"", ""OD Visualization"", ""Edge Bundling"", ""Trajectory""]","OD bundling is a promising method to identify key origin-destination (OD) patterns, but the bundling can mislead the interpretation of actual trajectories traveled. We present OD Morphing, an interactive OD bundling technique that improves geographical faithfulness to actual trajectories while preserving visual simplicity for OD patterns. OD Morphing iteratively identifies critical waypoints from the actual trajectory network with a min-cut algorithm and transitions OD bundles to pass through the identified waypoints with a smooth morphing method. Furthermore, we extend OD Morphing to support bundling at interaction speeds to enable users to interactively transition between degrees of faithfulness to aid sensemaking. We introduce metrics for faithfulness and simplicity to evaluate their trade-off achieved by OD morphed bundling. We demonstrate OD Morphing on real-world city-scale taxi trajectory and USA domestic planned flight datasets."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934812,JOUR,Uncertainty-Aware Principal Component Analysis,IEEE Transactions on Visualization and Computer Graphics,822,831,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934812,"['Gortler, J.', 'Spinner, T.', 'Streeb, D.', 'Weiskopf, D.', 'Deussen, O.']","[""Principal component analysis"", ""Uncertainty"", ""Dimensionality reduction"", ""Probability distribution"", ""Data visualization"", ""Covariance matrices"", ""Random variables"", ""Uncertainty"", ""dimensionality reduction"", ""principal component analysis"", ""linear projection"", ""machine learning""]","We present a technique to perform dimensionality reduction on data that is subject to uncertainty. Our method is a generalization of traditional principal component analysis (PCA) to multivariate probability distributions. In comparison to non-linear methods, linear dimensionality reduction techniques have the advantage that the characteristics of such probability distributions remain intact after projection. We derive a representation of the PCA sample covariance matrix that respects potential uncertainty in each of the inputs, building the mathematical foundation of our new method: uncertainty-aware PCA. In addition to the accuracy and performance gained by our approach over sampling-based strategies, our formulation allows us to perform sensitivity analysis with regard to the uncertainty in the data. For this, we propose factor traces as a novel visualization that enables to better understand the influence of uncertainty on the chosen principal components. We provide multiple examples of our technique using real-world datasets. As a special case, we show how to propagate multivariate normal distributions through PCA in closed form. Furthermore, we discuss extensions and limitations of our approach."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934242,JOUR,A Structural Average of Labeled Merge Trees for Uncertainty Visualization,IEEE Transactions on Visualization and Computer Graphics,832,842,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934242,"['Yan, L.', 'Wang, Y.', 'Munch, E.', 'Gasparovic, E.', 'Wang, B.']","[""Uncertainty"", ""Data visualization"", ""Vegetation"", ""Topology"", ""Visualization"", ""Measurement uncertainty"", ""Topological data analysis"", ""uncertainty visualization"", ""merge trees""]","Physical phenomena in science and engineering are frequently modeled using scalar fields. In scalar field topology, graph-based topological descriptors such as merge trees, contour trees, and Reeb graphs are commonly used to characterize topological changes in the (sub)level sets of scalar fields. One of the biggest challenges and opportunities to advance topology-based visualization is to understand and incorporate uncertainty into such topological descriptors to effectively reason about their underlying data. In this paper, we study a structural average of a set of labeled merge trees and use it to encode uncertainty in data. Specifically, we compute a 1-center tree that minimizes its maximum distance to any other tree in the set under a well-defined metric called the interleaving distance. We provide heuristic strategies that compute structural averages of merge trees whose labels do not fully agree. We further provide an interactive visualization system that resembles a numerical calculator that takes as input a set of merge trees and outputs a tree as their structural average. We also highlight structural similarities between the input and the average and incorporate uncertainty information for visual exploration. We develop a novel measure of uncertainty, referred to as consistency, via a metric-space view of the input trees. Finally, we demonstrate an application of our framework through merge trees that arise from ensembles of scalar fields. Our work is the first to employ interleaving distances and consistency to study a global, mathematically rigorous, structural average of merge trees in the context of uncertainty visualization."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934333,JOUR,Multiscale Visual Drilldown for the Analysis of Large Ensembles of Multi-Body Protein Complexes,IEEE Transactions on Visualization and Computer Graphics,843,852,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934333,"['Furmanova, K.', 'Jur@ik, A.', 'Kozlikova, B.', 'Hauser, H.', 'Byska, J.']","[""Proteins"", ""Tools"", ""Visualization"", ""Amino acids"", ""Proteomics"", ""Task analysis"", ""Space exploration"", ""Molecular visualization"", ""data filtering"", ""coordinated and multiple views""]","When studying multi-body protein complexes, biochemists use computational tools that can suggest hundreds or thousands of their possible spatial configurations. However, it is not feasible to experimentally verify more than only a very small subset of them. In this paper, we propose a novel multiscale visual drilldown approach that was designed in tight collaboration with proteomic experts, enabling a systematic exploration of the configuration space. Our approach takes advantage of the hierarchical structure of the data @ from the whole ensemble of protein complex configurations to the individual configurations, their contact interfaces, and the interacting amino acids. Our new solution is based on interactively linked 2D and 3D views for individual hierarchy levels. At each level, we offer a set of selection and filtering operations that enable the user to narrow down the number of configurations that need to be manually scrutinized. Furthermore, we offer a dedicated filter interface, which provides the users with an overview of the applied filtering operations and enables them to examine their impact on the explored ensemble. This way, we maintain the history of the exploration process and thus enable the user to return to an earlier point of the exploration. We demonstrate the effectiveness of our approach on two case studies conducted by collaborating proteomic experts."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934399,JOUR,Illusion of Causality in Visualized Data,IEEE Transactions on Visualization and Computer Graphics,853,862,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934399,"['Xiong, C.', 'Shapiro, J.', 'Hullman, J.', 'Franconeri, S.']","[""Correlation"", ""Data visualization"", ""Bars"", ""Task analysis"", ""Visualization"", ""Cognition"", ""Encoding"", ""Information Visualization"", ""Correlation and Causation"", ""Visualization Design"", ""Reasoning Affordance""]","Students who eat breakfast more frequently tend to have a higher grade point average. From this data, many people might confidently state that a before-school breakfast program would lead to higher grades. This is a reasoning error, because correlation does not necessarily indicate causation @ X and Y can be correlated without one directly causing the other. While this error is pervasive, its prevalence might be amplified or mitigated by the way that the data is presented to a viewer. Across three crowdsourced experiments, we examined whether how simple data relations are presented would mitigate this reasoning error. The first experiment tested examples similar to the breakfast-GPA relation, varying in the plausibility of the causal link. We asked participants to rate their level of agreement that the relation was correlated, which they rated appropriately as high. However, participants also expressed high agreement with a causal interpretation of the data. Levels of support for the causal interpretation were not equally strong across visualization types: causality ratings were highest for text descriptions and bar graphs, but weaker for scatter plots. But is this effect driven by bar graphs aggregating data into two groups or by the visual encoding type? We isolated data aggregation versus visual encoding type and examined their individual effect on perceived causality. Overall, different visualization designs afford different cognitive reasoning affordances across the same data. High levels of data aggregation by graphs tend to be associated with higher perceived causality in data. Participants perceived line and dot visual encodings as more causal than bar encodings. Our results demonstrate how some visualization designs trigger stronger causal links while choosing others can help mitigate unwarranted perceptions of causality."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934261,JOUR,"Ablate, Variate, and Contemplate: Visual Analytics for Discovering Neural Architectures",IEEE Transactions on Visualization and Computer Graphics,863,873,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934261,"['Cashman, D.', 'Perer, A.', 'Chang, R.', 'Strobelt, H.']","[""Neural networks"", ""Computer architecture"", ""Tools"", ""Training"", ""Visual analytics"", ""Machine learning"", ""visual analytics"", ""neural networks"", ""parameter space exploration""]","The performance of deep learning models is dependent on the precise configuration of many layers and parameters. However, there are currently few systematic guidelines for how to configure a successful model. This means model builders often have to experiment with different configurations by manually programming different architectures (which is tedious and time consuming) or rely on purely automated approaches to generate and train the architectures (which is expensive). In this paper, we present Rapid Exploration of Model Architectures and Parameters, or REMAP, a visual analytics tool that allows a model builder to discover a deep learning model quickly via exploration and rapid experimentation of neural network architectures. In REMAP, the user explores the large and complex parameter space for neural network architectures using a combination of global inspection and local experimentation. Through a visual overview of a set of models, the user identifies interesting clusters of architectures. Based on their findings, the user can run ablation and variation experiments to identify the effects of adding, removing, or replacing layers in a given architecture and generate new models accordingly. They can also handcraft new models using a simple graphical interface. As a result, a model builder can build deep learning models quickly, efficiently, and without manual programming. We inform the design of REMAP through a design study with four deep learning model builders. Through a use case, we demonstrate that REMAP allows users to discover performant neural network architectures efficiently using visual exploration and user-defined semi-automated searches through the model space."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934266,JOUR,VASSL: A Visual Analytics Toolkit for Social Spambot Labeling,IEEE Transactions on Visualization and Computer Graphics,874,883,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934266,"['Khayat, M.', 'Karimzadeh, M.', 'Zhao, J.', 'Ebert, D. S.']","[""Labeling"", ""Social networking (online)"", ""Visual analytics"", ""Feature extraction"", ""Tools"", ""Manuals"", ""Spambot"", ""Labeling"", ""Detection"", ""Visual Analytics"", ""Social Media Annotation""]","Social media platforms are filled with social spambots. Detecting these malicious accounts is essential, yet challenging, as they continually evolve to evade detection techniques. In this article, we present VASSL, a visual analytics system that assists in the process of detecting and labeling spambots. Our tool enhances the performance and scalability of manual labeling by providing multiple connected views and utilizing dimensionality reduction, sentiment analysis and topic modeling, enabling insights for the identification of spambots. The system allows users to select and analyze groups of accounts in an interactive manner, which enables the detection of spambots that may not be identified when examined individually. We present a user study to objectively evaluate the performance of VASSL users, as well as capturing subjective opinions about the usefulness and the ease of use of the tool."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934595,JOUR,Visual Interaction with Deep Learning Models through Collaborative Semantic Inference,IEEE Transactions on Visualization and Computer Graphics,884,894,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934595,"['Gehrmann, S.', 'Strobelt, H.', 'Kruger, R.', 'Pfister, H.', 'Rush, A. M.']","[""Visualization"", ""Collaboration"", ""Semantics"", ""Tools"", ""Analytical models"", ""Cognition"", ""Predictive models"", ""Human-Computer Collaboration"", ""Deep Learning"", ""Neural Networks"", ""Interaction Design"", ""Human-Centered Design""]","Automation of tasks can have critical consequences when humans lose agency over decision processes. Deep learning models are particularly susceptible since current black-box approaches lack explainable reasoning. We argue that both the visual interface and model structure of deep learning systems need to take into account interaction design. We propose a framework of collaborative semantic inference (CSI) for the co-design of interactions and models to enable visual collaboration between humans and algorithms. The approach exposes the intermediate reasoning process of models which allows semantic interactions with the visual metaphors of a problem, which means that a user can both understand and control parts of the model reasoning process. We demonstrate the feasibility of CSI with a co-designed case study of a document summarization system."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934398,JOUR,DataShot: Automatic Generation of Fact Sheets from Tabular Data,IEEE Transactions on Visualization and Computer Graphics,895,905,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934398,"['Wang, Y.', 'Sun, Z.', 'Zhang, H.', 'Cui, W.', 'Xu, K.', 'Ma, X.', 'Zhang, D.']","[""Data visualization"", ""Visualization"", ""Data mining"", ""Tools"", ""Shape"", ""Sun"", ""Data analysis"", ""Fact sheet"", ""infographic"", ""visualization"", ""and automated design""]","Fact sheets with vivid graphical design and intriguing statistical insights are prevalent for presenting raw data. They help audiences understand data-related facts effectively and make a deep impression. However, designing a fact sheet requires both data and design expertise and is a laborious and time-consuming process. One needs to not only understand the data in depth but also produce intricate graphical representations. To assist in the design process, we present DataShot which, to the best of our knowledge, is the first automated system that creates fact sheets automatically from tabular data. First, we conduct a qualitative analysis of 245 infographic examples to explore general infographic design space at both the sheet and element levels. We identify common infographic structures, sheet layouts, fact types, and visualization styles during the study. Based on these findings, we propose a fact sheet generation pipeline, consisting of fact extraction, fact composition, and presentation synthesis, for the auto-generation workflow. To validate our system, we present use cases with three real-world datasets. We conduct an in-lab user study to understand the usage of our system. Our evaluation results show that DataShot can efficiently generate satisfactory fact sheets to support further customization and data presentation."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934785,JOUR,Text-to-Viz: Automatic Generation of Infographics from Proportion-Related Natural Language Statements,IEEE Transactions on Visualization and Computer Graphics,906,916,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934785,"['Cui, W.', 'Zhang, X.', 'Wang, Y.', 'Huang, H.', 'Chen, B.', 'Fang, L.', 'Zhang, H.', 'Lou, J.', 'Zhang, D.']","[""Data visualization"", ""Visualization"", ""Tools"", ""Sports"", ""Authoring systems"", ""Natural languages"", ""Social networking (online)"", ""Visualization for the masses"", ""infographic"", ""automatic visualization"", ""presentation"", ""and dissemination""]","Combining data content with visual embellishments, infographics can effectively deliver messages in an engaging and memorable manner. Various authoring tools have been proposed to facilitate the creation of infographics. However, creating a professional infographic with these authoring tools is still not an easy task, requiring much time and design expertise. Therefore, these tools are generally not attractive to casual users, who are either unwilling to take time to learn the tools or lacking in proper design expertise to create a professional infographic. In this paper, we explore an alternative approach: to automatically generate infographics from natural language statements. We first conducted a preliminary study to explore the design space of infographics. Based on the preliminary study, we built a proof-of-concept system that automatically converts statements about simple proportion-related statistics to a set of infographics with pre-designed styles. Finally, we demonstrated the usability and usefulness of the system through sample results, exhibits, and expert reviews."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934810,JOUR,Towards Automated Infographic Design: Deep Learning-based Auto-Extraction of Extensible Timeline,IEEE Transactions on Visualization and Computer Graphics,917,926,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934810,"['Chen, Z.', 'Wang, Y.', 'Wang, Q.', 'Wang, Y.', 'Qu, H.']","[""Visualization"", ""Data visualization"", ""Data mining"", ""Image reconstruction"", ""Pipelines"", ""Image coding"", ""Object detection"", ""Automated Infographic Design"", ""Deep Learning-based Approach"", ""Timeline Infographics"", ""Multi-task Model""]","Designers need to consider not only perceptual effectiveness but also visual styles when creating an infographic. This process can be difficult and time consuming for professional designers, not to mention non-expert users, leading to the demand for automated infographics design. As a first step, we focus on timeline infographics, which have been widely used for centuries. We contribute an end-to-end approach that automatically extracts an extensible timeline template from a bitmap image. Our approach adopts a deconstruction and reconstruction paradigm. At the deconstruction stage, we propose a multi-task deep neural network that simultaneously parses two kinds of information from a bitmap timeline: 1) the global information, i.e., the representation, scale, layout, and orientation of the timeline, and 2) the local information, i.e., the location, category, and pixels of each visual element on the timeline. At the reconstruction stage, we propose a pipeline with three techniques, i.e., Non-Maximum Merging, Redundancy Recover, and DL GrabCut, to extract an extensible template from the infographic, by utilizing the deconstruction results. To evaluate the effectiveness of our approach, we synthesize a timeline dataset (4296 images) and collect a real-world timeline dataset (393 images) from the Internet. We first report quantitative evaluation results of our approach over the two datasets. Then, we present examples of automatically extracted templates and timelines automatically generated based on these templates to qualitatively demonstrate the performance. The results confirm that our approach can effectively extract extensible templates from real-world timeline infographics."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934656,JOUR,EmoCo: Visual Analysis of Emotion Coherence in Presentation Videos,IEEE Transactions on Visualization and Computer Graphics,927,937,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934656,"['Zeng, H.', 'Wang, X.', 'Wu, A.', 'Wang, Y.', 'Li, Q.', 'Endert, A.', 'Qu, H.']","[""Coherence"", ""Videos"", ""Feature extraction"", ""Visual analytics"", ""Emotion recognition"", ""Face"", ""Emotion"", ""coherence"", ""video analysis"", ""visual analysis""]","Emotions play a key role in human communication and public presentations. Human emotions are usually expressed through multiple modalities. Therefore, exploring multimodal emotions and their coherence is of great value for understanding emotional expressions in presentations and improving presentation skills. However, manually watching and studying presentation videos is often tedious and time-consuming. There is a lack of tool support to help conduct an efficient and in-depth multi-level analysis. Thus, in this paper, we introduce EmoCo, an interactive visual analytics system to facilitate efficient analysis of emotion coherence across facial, text, and audio modalities in presentation videos. Our visualization system features a channel coherence view and a sentence clustering view that together enable users to obtain a quick overview of emotion coherence and its temporal evolution. In addition, a detail view and word view enable detailed exploration and comparison from the sentence level and word level, respectively. We thoroughly evaluate the proposed system and visualization techniques through two usage scenarios based on TED Talk videos and interviews with two domain experts. The results demonstrate the effectiveness of our system in gaining insights into emotion coherence in presentations."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934402,JOUR,CerebroVis: Designing an Abstract yet Spatially Contextualized Cerebral Artery Network Visualization,IEEE Transactions on Visualization and Computer Graphics,938,948,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934402,"['Pandey, A.', 'Shukla, H.', 'Young, G. S.', 'Qin, L.', 'Zamani, A. A.', 'Hsu, L.', 'Huang, R.', 'Dunne, C.', 'Borkin, M. A.']","[""Arteries"", ""Blood"", ""Visualization"", ""Three-dimensional displays"", ""Two dimensional displays"", ""Task analysis"", ""Imaging"", ""Network Visualization"", ""Spatial Context"", ""Abstract Design"", ""Flow Network"", ""Medical Imaging"", ""Cerebral Arteries""]","Blood circulation in the human brain is supplied through a network of cerebral arteries. If a clinician suspects a patient has a stroke or other cerebrovascular condition, they order imaging tests. Neuroradiologists visually search the resulting scans for abnormalities. Their visual search tasks correspond to the abstract network analysis tasks of browsing and path following. To assist neuroradiologists in identifying cerebral artery abnormalities, we designed CerebroVis, a novel abstract--yet spatially contextualized--cerebral artery network visualization. In this design study, we contribute a novel framing and definition of the cerebral artery system in terms of network theory and characterize neuroradiologist domain goals as abstract visualization and network analysis tasks. Through an iterative, user-centered design process we developed an abstract network layout technique which incorporates cerebral artery spatial context. The abstract visualization enables increased domain task performance over 3D geometry representations, while including spatial context helps preserve the user's mental map of the underlying geometry. We provide open source implementations of our network layout technique and prototype cerebral artery visualization tool. We demonstrate the robustness of our technique by successfully laying out 61 open source brain scans. We evaluate the effectiveness of our layout through a mixed methods study with three neuroradiologists. In a formative controlled experiment our study participants used CerebroVis and a conventional 3D visualization to examine real cerebral artery imaging data to identify a simulated intracranial artery stenosis. Participants were more accurate at identifying stenoses using CerebroVis (absolute risk difference 13%). A free copy of this paper, the evaluation stimuli and data, and source code are available at osf.io/e5sxt."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934546,JOUR,Cohort-based T-SSIM Visual Computing for Radiation Therapy Prediction and Exploration,IEEE Transactions on Visualization and Computer Graphics,949,959,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934546,"['Wentzel, A.', 'Hanula, P.', 'Luciani, T.', 'Elgohari, B.', 'Elhalawani, H.', 'Canahuate, G.', 'Vock, D.', 'Fuller, C. D.', 'Marai, G. E.']","[""Visualization"", ""Planning"", ""Tumors"", ""Biological systems"", ""Data visualization"", ""Prediction algorithms"", ""Biomedical applications of radiation"", ""Biomedical and Medical Visualization"", ""Spatial Techniques"", ""Visual Design"", ""High-Dimensional Data""]","We describe a visual computing approach to radiation therapy (RT) planning, based on spatial similarity within a patient cohort. In radiotherapy for head and neck cancer treatment, dosage to organs at risk surrounding a tumor is a large cause of treatment toxicity. Along with the availability of patient repositories, this situation has lead to clinician interest in understanding and predicting RT outcomes based on previously treated similar patients. To enable this type of analysis, we introduce a novel topology-based spatial similarity measure, T-SSIM, and a predictive algorithm based on this similarity measure. We couple the algorithm with a visual steering interface that intertwines visual encodings for the spatial data and statistical results, including a novel parallel-marker encoding that is spatially aware. We report quantitative results on a cohort of 165 patients, as well as a qualitative evaluation with domain experts in radiation oncology, data management, biostatistics, and medical imaging, who are collaborating remotely."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934369,JOUR,DeepOrganNet: On-the-Fly Reconstruction and Visualization of 3D / 4D Lung Models from Single-View Projections by Deep Deformation Network,IEEE Transactions on Visualization and Computer Graphics,960,970,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934369,"['Wang, Y.', 'Zhong, Z.', 'Hua, J.']","[""Three-dimensional displays"", ""Image reconstruction"", ""Shape"", ""Strain"", ""Solid modeling"", ""Biomedical imaging"", ""Two dimensional displays"", ""Deep deformation network"", ""organ meshes"", ""3D / 4D shapes"", ""2D projections"", ""single-view""]","This paper introduces a deep neural network based method, i.e., DeepOrganNet, to generate and visualize fully high-fidelity 3D / 4D organ geometric models from single-view medical images with complicated background in real time. Traditional 3D / 4D medical image reconstruction requires near hundreds of projections, which cost insufferable computational time and deliver undesirable high imaging / radiation dose to human subjects. Moreover, it always needs further notorious processes to segment or extract the accurate 3D organ models subsequently. The computational time and imaging dose can be reduced by decreasing the number of projections, but the reconstructed image quality is degraded accordingly. To our knowledge, there is no method directly and explicitly reconstructing multiple 3D organ meshes from a single 2D medical grayscale image on the fly. Given single-view 2D medical images, e.g., 3D / 4D-CT projections or X-ray images, our end-to-end DeepOrganNet framework can efficiently and effectively reconstruct 3D / 4D lung models with a variety of geometric shapes by learning the smooth deformation fields from multiple templates based on a trivariate tensor-product deformation technique, leveraging an informative latent descriptor extracted from input 2D images. The proposed method can guarantee to generate high-quality and high-fidelity manifold meshes for 3D / 4D lung models; while, all current deep learning based approaches on the shape reconstruction from a single image cannot. The major contributions of this work are to accurately reconstruct the 3D organ shapes from 2D single-view projection, significantly improve the procedure time to allow on-the-fly visualization, and dramatically reduce the imaging dose for human subjects. Experimental results are evaluated and compared with the traditional reconstruction method and the state-of-the-art in deep learning, by using extensive 3D and 4D examples, including both synthetic phantom and real patient datasets. The efficiency of the proposed method shows that it only needs several milliseconds to generate organ meshes with 10K vertices, which has great potential to be used in real-time image guided radiation therapy (IGRT)."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934337,JOUR,Temporal Views of Flattened Mitral Valve Geometries,IEEE Transactions on Visualization and Computer Graphics,971,980,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934337,"['Eulzer, P.', 'Engelhardt, S.', 'Lichtenberg, N.', 'de Simone, R.', 'Lawonn, K.']","[""Valves"", ""Three-dimensional displays"", ""Data visualization"", ""Surgery"", ""Two dimensional displays"", ""Geometry"", ""Solid modeling"", ""Mitral valve"", ""quantification"", ""medical visualization"", ""parameterization"", ""spatio-temporal visualization""]","The mitral valve, one of the four valves in the human heart, controls the bloodflow between the left atrium and ventricle and may suffer from various pathologies. Malfunctioning valves can be treated by reconstructive surgeries, which have to be carefully planned and evaluated. While current research focuses on the modeling and segmentation of the valve, we base our work on existing segmentations of patient-specific mitral valves, that are also time-resolved ($3\mathrm{D}+\mathrm{t}$) over the cardiac cycle. The interpretation of the data can be ambiguous, due to the complex surface of the valve and multiple time steps. We therefore propose a software prototype to analyze such $3\mathrm{D}+\mathrm{t}$ data, by extracting pathophysiological parameters and presenting them via dimensionally reduced visualizations. For this, we rely on an existing algorithm to unroll the convoluted valve surface towards a flattened 2D representation. In this paper, we show that the $3\mathrm{D}+\mathrm{t}$ data can be transferred to 3D or 2D representations in a way that allows the domain expert to faithfully grasp important aspects of the cardiac cycle. In this course, we not only consider common pathophysiological parameters, but also introduce new observations that are derived from landmarks within the segmentation model. Our analysis techniques were developed in collaboration with domain experts and a survey showed that the insights have the potential to support mitral valve diagnosis and the comparison of the pre- and post-operative condition of a patient."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934280,JOUR,Motion Browser: Visualizing and Understanding Complex Upper Limb Movement Under Obstetrical Brachial Plexus Injuries,IEEE Transactions on Visualization and Computer Graphics,981,990,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934280,"['Chan, G. Y.', 'Nonato, L. G.', 'Chu, A.', 'Raghavan, P.', 'Aluru, V.', 'Silva, C. T.']","[""Muscles"", ""Data visualization"", ""Brachytherapy"", ""Time series analysis"", ""Task analysis"", ""Visual analytics"", ""Medical Data Visualization"", ""Visual Analytics Application"", ""Time Series Data"", ""Multimodal Data"", ""Brachial Plexus Injuries""]","The brachial plexus is a complex network of peripheral nerves that enables sensing from and control of the movements of the arms and hand. Nowadays, the coordination between the muscles to generate simple movements is still not well understood, hindering the knowledge of how to best treat patients with this type of peripheral nerve injury. To acquire enough information for medical data analysis, physicians conduct motion analysis assessments with patients to produce a rich dataset of electromyographic signals from multiple muscles recorded with joint movements during real-world tasks. However, tools for the analysis and visualization of the data in a succinct and interpretable manner are currently not available. Without the ability to integrate, compare, and compute multiple data sources in one platform, physicians can only compute simple statistical values to describe patient's behavior vaguely, which limits the possibility to answer clinical questions and generate hypotheses for research. To address this challenge, we have developed Motion Browser, an interactive visual analytics system which provides an efficient framework to extract and compare muscle activity patterns from the patient's limbs and coordinated views to help users analyze muscle signals, motion data, and video information to address different tasks. The system was developed as a result of a collaborative endeavor between computer scientists and orthopedic surgery and rehabilitation physicians. We present case studies showing physicians can utilize the information displayed to understand how individuals coordinate their muscles to initiate appropriate treatment and generate new hypotheses for future research."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934783,JOUR,ShapeWordle: Tailoring Wordles using Shape-aware Archimedean Spirals,IEEE Transactions on Visualization and Computer Graphics,991,1000,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934783,"['Wang, Y.', 'Chu, X.', 'Zhang, K.', 'Bao, C.', 'Li, X.', 'Zhang, J.', 'Fu, C.', 'Hurter, C.', 'Deussen, O.', 'Lee, B.']","[""Shape"", ""Layout"", ""Spirals"", ""Tag clouds"", ""Image color analysis"", ""Tools"", ""Semantics"", ""Wordle"", ""Archimedean spiral"", ""shape""]","We present a new technique to enable the creation of shape-bounded Wordles, we call ShapeWordle, in which we fit words to form a given shape. To guide word placement within a shape, we extend the traditional Archimedean spirals to be shape-aware by formulating the spirals in a differential form using the distance field of the shape. To handle non-convex shapes, we introduce a multi-centric Wordle layout method that segments the shape into parts for our shape-aware spirals to adaptively fill the space and generate word placements. In addition, we offer a set of editing interactions to facilitate the creation of semantically-meaningful Wordles. Lastly, we present three evaluations: a comprehensive comparison of our results against the state-of-the-art technique (WordArt), case studies with 14 users, and a gallery to showcase the coverage of our technique."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934654,JOUR,Semantic Concept Spaces: Guided Topic Model Refinement using Word-Embedding Projections,IEEE Transactions on Visualization and Computer Graphics,1001,1011,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934654,"['El-Assady, M.', 'Kehlbeck, R.', 'Collins, C.', 'Keim, D.', 'Deussen, O.']","[""Semantics"", ""Analytical models"", ""Computational modeling"", ""Visual analytics"", ""Machine learning"", ""Task analysis"", ""Topic Model Optimization"", ""Word Embedding"", ""Mixed-Initiative Refinement"", ""Guided Visual Analytics"", ""Semantic Mapping""]","We present a framework that allows users to incorporate the semantics of their domain knowledge for topic model refinement while remaining model-agnostic. Our approach enables users to (1) understand the semantic space of the model, (2) identify regions of potential conflicts and problems, and (3) readjust the semantic relation of concepts based on their understanding, directly influencing the topic modeling. These tasks are supported by an interactive visual analytics workspace that uses word-embedding projections to define concept regions which can then be refined. The user-refined concepts are independent of a particular document collection and can be transferred to related corpora. All user interactions within the concept space directly affect the semantic relations of the underlying vector space model, which, in turn, change the topic modeling. In addition to direct manipulation, our system guides the users' decision-making process through recommended interactions that point out potential improvements. This targeted refinement aims at minimizing the feedback required for an efficient human-in-the-loop process. We confirm the improvements achieved through our approach in two user studies that show topic model quality improvements through our visual knowledge externalization and learning process."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934786,JOUR,The Perceptual Proxies of Visual Comparison,IEEE Transactions on Visualization and Computer Graphics,1012,1021,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934786,"['Jardine, N.', 'Ondov, B. D.', 'Elmqvist, N.', 'Franconeri, S.']","[""Visualization"", ""Task analysis"", ""Bars"", ""Correlation"", ""Data visualization"", ""Visual systems"", ""Animation"", ""Graphical perception"", ""visual perception"", ""visual comparison"", ""crowdsourced evaluation""]","Perceptual tasks in visualizations often involve comparisons. Of two sets of values depicted in two charts, which set had values that were the highest overall? Which had the widest range? Prior empirical work found that the performance on different visual comparison tasks (e.g., ""biggest delta@, ""biggest correlation@) varied widely across different combinations of marks and spatial arrangements. In this paper, we expand upon these combinations in an empirical evaluation of two new comparison tasks: the ""biggest mean@ and ""biggest range@ between two sets of values. We used a staircase procedure to titrate the difficulty of the data comparison to assess which arrangements produced the most precise comparisons for each task. We find visual comparisons of biggest mean and biggest range are supported by some chart arrangements more than others, and that this pattern is substantially different from the pattern for other tasks. To synthesize these dissonant findings, we argue that we must understand which features of a visualization are actually used by the human visual system to solve a given task. We call these perceptual proxies. For example, when comparing the means of two bar charts, the visual system might use a ""Mean length@ proxy that isolates the actual lengths of the bars and then constructs a true average across these lengths. Alternatively, it might use a ""Hull Area@ proxy that perceives an implied hull bounded by the bars of each chart and then compares the areas of these hulls. We propose a series of potential proxies across different tasks, marks, and spatial arrangements. Simple models of these proxies can be empirically evaluated for their explanatory power by matching their performance to human performance across these marks, arrangements, and tasks. We use this process to highlight candidates for perceptual proxies that might scale more broadly to explain performance in visual comparison."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934535,JOUR,BarcodeTree: Scalable Comparison of Multiple Hierarchies,IEEE Transactions on Visualization and Computer Graphics,1022,1032,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934535,"['Li, G.', 'Zhang, Y.', 'Dong, Y.', 'Liang, J.', 'Zhang, J.', 'Wang, J.', 'Mcguffin, M. J.', 'Yuan, X.']","[""Vegetation"", ""Visualization"", ""Encoding"", ""Task analysis"", ""Data visualization"", ""Layout"", ""Libraries"", ""tree visualization"", ""comparison"", ""multiple trees""]","We propose BarcodeTree (BCT), a novel visualization technique for comparing topological structures and node attribute values of multiple trees. BCT can provide an overview of one hundred shallow and stable trees simultaneously, without aggregating individual nodes. Each BCT is shown within a single row using a style similar to a barcode, allowing trees to be stacked vertically with matching nodes aligned horizontally to ease comparison and maintain space efficiency. We design several visual cues and interactive techniques to help users understand the topological structure and compare trees. In an experiment comparing two variants of BCT with icicle plots, the results suggest that BCTs make it easier to visually compare trees by reducing the vertical distance between different trees. We also present two case studies involving a dataset of hundreds of trees to demonstrate BCT's utility."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934784,JOUR,A Comparison of Radial and Linear Charts for Visualizing Daily Patterns,IEEE Transactions on Visualization and Computer Graphics,1033,1042,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934784,"['Waldner, M.', 'Diehl, A.', 'Gra@anin, D.', 'Splechtna, R.', 'Delrieux, C.', 'Matkovic, K.']","[""Bars"", ""Data visualization"", ""Layout"", ""Clocks"", ""Task analysis"", ""Time series analysis"", ""Encoding"", ""Radial charts"", ""time series series data"", ""daily patterns"", ""crowd-sourced experiment""]","Radial charts are generally considered less effective than linear charts. Perhaps the only exception is in visualizing periodical time-dependent data, which is believed to be naturally supported by the radial layout. It has been demonstrated that the drawbacks of radial charts outweigh the benefits of this natural mapping. Visualization of daily patterns, as a special case, has not been systematically evaluated using radial charts. In contrast to yearly or weekly recurrent trends, the analysis of daily patterns on a radial chart may benefit from our trained skill on reading radial clocks that are ubiquitous in our culture. In a crowd-sourced experiment with 92 non-expert users, we evaluated the accuracy, efficiency, and subjective ratings of radial and linear charts for visualizing daily traffic accident patterns. We systematically compared juxtaposed 12-hours variants and single 24-hours variants for both layouts in four low-level tasks and one high-level interpretation task. Our results show that over all tasks, the most elementary 24-hours linear bar chart is most accurate and efficient and is also preferred by the users. This provides strong evidence for the use of linear layouts @ even for visualizing periodical daily patterns."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934540,JOUR,Separating the Wheat from the Chaff: Comparative Visual Cues for Transparent Diagnostics of Competing Models,IEEE Transactions on Visualization and Computer Graphics,1043,1053,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934540,"['Dasgupta, A.', 'Wang, H.', ""O'Brien, N."", 'Burrows, S.']","[""Measurement"", ""Visualization"", ""Analytical models"", ""Task analysis"", ""Meteorology"", ""Data models"", ""Complexity theory"", ""Visual comparison"", ""Visual cues"", ""Model evaluation"", ""Transparency"", ""Simulation""]","Experts in data and physical sciences have to regularly grapple with the problem of competing models. Be it analytical or physics-based models, a cross-cutting challenge for experts is to reliably diagnose which model outcomes appropriately predict or simulate real-world phenomena. Expert judgment involves reconciling information across many, and often, conflicting criteria that describe the quality of model outcomes. In this paper, through a design study with climate scientists, we develop a deeper understanding of the problem and solution space of model diagnostics, resulting in the following contributions: i) a problem and task characterization using which we map experts' model diagnostics goals to multi-way visual comparison tasks, ii) a design space of comparative visual cues for letting experts quickly understand the degree of disagreement among competing models and gauge the degree of stability of model outputs with respect to alternative criteria, and iii) design and evaluation of MyriadCues, an interactive visualization interface for exploring alternative hypotheses and insights about good and bad models by leveraging comparative visual cues. We present case studies and subjective feedback by experts, which validate how MyriadCues enables more transparent model diagnostic mechanisms, as compared to the state of the art."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934289,JOUR,STBins: Visual Tracking and Comparison of Multiple Data Sequences Using Temporal Binning,IEEE Transactions on Visualization and Computer Graphics,1054,1063,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934289,"['Qi, J.', 'Bloemen, V.', 'Wang, S.', 'van Wijk, J.', 'van de Wetering, H.']","[""Visualization"", ""Data visualization"", ""Task analysis"", ""Message systems"", ""Mathematics"", ""Computer science"", ""Image color analysis"", ""Visualization"", ""time series data"", ""data sequence""]","While analyzing multiple data sequences, the following questions typically arise: how does a single sequence change over time, how do multiple sequences compare within a period, and how does such comparison change over time. This paper presents a visual technique named STBins to answer these questions. STBins is designed for visual tracking of individual data sequences and also for comparison of sequences. The latter is done by showing the similarity of sequences within temporal windows. A perception study is conducted to examine the readability of alternative visual designs based on sequence tracking and comparison tasks. Also, two case studies based on real-world datasets are presented in detail to demonstrate usage of our technique."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934629,JOUR,explAIner: A Visual Analytics Framework for Interactive and Explainable Machine Learning,IEEE Transactions on Visualization and Computer Graphics,1064,1074,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934629,"['Spinner, T.', 'Schlegel, U.', 'Schafer, H.', 'El-Assady, M.']","[""Data models"", ""Analytical models"", ""Computational modeling"", ""Pipelines"", ""Machine learning"", ""Monitoring"", ""Explainable AI"", ""Interactive Machine Learning"", ""Deep Learning"", ""Visual Analytics"", ""Interpretability"", ""Explainability""]","We propose a framework for interactive and explainable machine learning that enables users to (1) understand machine learning models; (2) diagnose model limitations using different explainable AI methods; as well as (3) refine and optimize the models. Our framework combines an iterative XAI pipeline with eight global monitoring and steering mechanisms, including quality monitoring, provenance tracking, model comparison, and trust building. To operationalize the framework, we present explAIner, a visual analytics system for interactive and explainable machine learning that instantiates all phases of the suggested pipeline within the commonly used TensorBoard environment. We performed a user-study with nine participants across different expertise levels to examine their perception of our workflow and to collect suggestions to fill the gap between our system and framework. The evaluation confirms that our tightly integrated system leads to an informed machine learning process while disclosing opportunities for further extensions."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934631,JOUR,Explaining Vulnerabilities to Adversarial Machine Learning through Visual Analytics,IEEE Transactions on Visualization and Computer Graphics,1075,1085,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934631,"['Ma, Y.', 'Xie, T.', 'Li, J.', 'Maciejewski, R.']","[""Analytical models"", ""Machine learning"", ""Visual analytics"", ""Training"", ""Predictive models"", ""Adversarial machine learning"", ""data poisoning"", ""visual analytics""]","Machine learning models are currently being deployed in a variety of real-world applications where model predictions are used to make decisions about healthcare, bank loans, and numerous other critical tasks. As the deployment of artificial intelligence technologies becomes ubiquitous, it is unsurprising that adversaries have begun developing methods to manipulate machine learning models to their advantage. While the visual analytics community has developed methods for opening the black box of machine learning models, little work has focused on helping the user understand their model vulnerabilities in the context of adversarial attacks. In this paper, we present a visual analytics framework for explaining and exploring model vulnerabilities to adversarial attacks. Our framework employs a multi-faceted visualization scheme designed to support the analysis of data poisoning attacks from the perspective of models, data instances, features, and local structures. We demonstrate our framework through two case studies on binary classifiers and illustrate model vulnerabilities with respect to varying attack strategies."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934262,JOUR,FairSight: Visual Analytics for Fairness in Decision Making,IEEE Transactions on Visualization and Computer Graphics,1086,1095,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934262,"['Ahn, Y.', 'Lin, Y.']","[""Decision making"", ""Machine learning"", ""Tools"", ""Pipelines"", ""Visual analytics"", ""Machine learning algorithms"", ""Task analysis"", ""Fairness in Machine Learning"", ""Visual Analytic""]","Data-driven decision making related to individuals has become increasingly pervasive, but the issue concerning the potential discrimination has been raised by recent studies. In response, researchers have made efforts to propose and implement fairness measures and algorithms, but those efforts have not been translated to the real-world practice of data-driven decision making. As such, there is still an urgent need to create a viable tool to facilitate fair decision making. We propose FairSight, a visual analytic system to address this need; it is designed to achieve different notions of fairness in ranking decisions through identifying the required actions @ understanding, measuring, diagnosing and mitigating biases @ that together lead to fairer decision making. Through a case study and user study, we demonstrate that the proposed visual analytic and diagnostic modules in the system are effective in understanding the fairness-aware decision pipeline and obtaining more fair outcomes."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934659,JOUR,Summit: Scaling Deep Learning Interpretability by Visualizing Activation and Attribution Summarizations,IEEE Transactions on Visualization and Computer Graphics,1096,1106,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934659,"['Hohman, F.', 'Park, H.', 'Robinson, C.', 'Chau, D. H. Polo']","[""Neurons"", ""Biological neural networks"", ""Feature extraction"", ""Data visualization"", ""Computational modeling"", ""Predictive models"", ""Visualization"", ""Deep learning interpretability"", ""visual analytics"", ""scalable summarization"", ""attribution graph""]","Deep learning is increasingly used in decision-making tasks. However, understanding how neural networks produce final predictions remains a fundamental challenge. Existing work on interpreting neural network predictions for images often focuses on explaining predictions for single images or neurons. As predictions are often computed from millions of weights that are optimized over millions of images, such explanations can easily miss a bigger picture. We present Summit, an interactive system that scalably and systematically summarizes and visualizes what features a deep learning model has learned and how those features interact to make predictions. Summit introduces two new scalable summarization techniques: (1) activation aggregation discovers important neurons, and (2) neuron-influence aggregation identifies relationships among such neurons. Summit combines these techniques to create the novel attribution graph that reveals and summarizes crucial neuron associations and substructures that contribute to a model's outcomes. Summit scales to large data, such as the ImageNet dataset with 1.2M images, and leverages neural network feature visualization and dataset examples to help users distill large, complex neural network models into compact, interactive visualizations. We present neural network exploration scenarios where Summit helps us discover multiple surprising insights into a prevalent, large-scale image classifier's learned representations and informs future neural network architecture design. The Summit visualization runs in modern web browsers and is open-sourced."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934613,JOUR,CloudDet: Interactive Visual Analysis of Anomalous Performances in Cloud Computing Systems,IEEE Transactions on Visualization and Computer Graphics,1107,1117,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934613,"['Xu, K.', 'Wang, Y.', 'Yang, L.', 'Wang, Y.', 'Qiao, B.', 'Qin, S.', 'Xu, Y.', 'Zhang, H.', 'Qu, H.']","[""Cloud computing"", ""Data visualization"", ""Anomaly detection"", ""Measurement"", ""Time series analysis"", ""Visualization"", ""Principal component analysis"", ""Cloud computing"", ""anomaly detection"", ""multidimensional data"", ""performance visualization"", ""visual analytics""]","Detecting and analyzing potential anomalous performances in cloud computing systems is essential for avoiding losses to customers and ensuring the efficient operation of the systems. To this end, a variety of automated techniques have been developed to identify anomalies in cloud computing. These techniques are usually adopted to track the performance metrics of the system (e.g., CPU, memory, and disk I/O), represented by a multivariate time series. However, given the complex characteristics of cloud computing data, the effectiveness of these automated methods is affected. Thus, substantial human judgment on the automated analysis results is required for anomaly interpretation. In this paper, we present a unified visual analytics system named CloudDet to interactively detect, inspect, and diagnose anomalies in cloud computing systems. A novel unsupervised anomaly detection algorithm is developed to identify anomalies based on the specific temporal patterns of the given metrics data (e.g., the periodic pattern). Rich visualization and interaction designs are used to help understand the anomalies in the spatial and temporal context. We demonstrate the effectiveness of CloudDet through a quantitative evaluation, two case studies with real-world data, and interviews with domain experts."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934285,JOUR,Visualizing a Moving Target: A Design Study on Task Parallel Programs in the Presence of Evolving Data and Concerns,IEEE Transactions on Visualization and Computer Graphics,1118,1128,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934285,"['Williams, K.', 'Bigelow, A.', 'Isaacs, K.']","[""Task analysis"", ""Data visualization"", ""Computational modeling"", ""Runtime"", ""Tools"", ""Parallel processing"", ""Data collection"", ""design studies"", ""software visualization"", ""parallel computing"", ""graph visualization""]","Common pitfalls in visualization projects include lack of data availability and the domain users' needs and focus changing too rapidly for the design process to complete. While it is often prudent to avoid such projects, we argue it can be beneficial to engage them in some cases as the visualization process can help refine data collection, solving a ""chicken and egg@ problem of having the data and tools to analyze it. We found this to be the case in the domain of task parallel computing where such data and tooling is an open area of research. Despite these hurdles, we conducted a design study. Through a tightly-coupled iterative design process, we built Atria, a multi-view execution graph visualization to support performance analysis. Atria simplifies the initial representation of the execution graph by aggregating nodes as related to their line of code. We deployed Atria on multiple platforms, some requiring design alteration. We describe how we adapted the design study methodology to the ""moving target@ of both the data and the domain experts' concerns and how this movement kept both the visualization and programming project healthy. We reflect on our process and discuss what factors allow the project to be successful in the presence of changing data and user needs."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934669,JOUR,Exploranative Code Quality Documents,IEEE Transactions on Visualization and Computer Graphics,1129,1139,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934669,"['Mumtaz, H.', 'Latif, S.', 'Beck, F.', 'Weiskopf, D.']","[""Data visualization"", ""Visual analytics"", ""Software"", ""Software metrics"", ""Software engineering"", ""Code quality"", ""interactive documents"", ""natural language generation"", ""sparklines""]","Good code quality is a prerequisite for efficiently developing maintainable software. In this paper, we present a novel approach to generate exploranative (explanatory and exploratory) data-driven documents that report code quality in an interactive, exploratory environment. We employ a template-based natural language generation method to create textual explanations about the code quality, dependent on data from software metrics. The interactive document is enriched by different kinds of visualization, including parallel coordinates plots and scatterplots for data exploration and graphics embedded into text. We devise an interaction model that allows users to explore code quality with consistent linking between text and visualizations; through integrated explanatory text, users are taught background knowledge about code quality aspects. Our approach to interactive documents was developed in a design study process that included software engineering and visual analytics experts. Although the solution is specific to the software engineering scenario, we discuss how the concept could generalize to multivariate data and report lessons learned in a broader scope."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934557,JOUR,OntoPlot: A Novel Visualisation for Non-hierarchical Associations in Large Ontologies,IEEE Transactions on Visualization and Computer Graphics,1140,1150,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934557,"['Yang, Y.', 'Wybrow, M.', 'Li, Y.', 'Czauderna, T.', 'He, Y.']","[""Ontologies"", ""Visualization"", ""Drugs"", ""Task analysis"", ""Data visualization"", ""Tools"", ""Biology"", ""Ontology visualisation"", ""visual compression"", ""interactive exploration"", ""ontology associations""]","Ontologies are formal representations of concepts and complex relationships among them. They have been widely used to capture comprehensive domain knowledge in areas such as biology and medicine, where large and complex ontologies can contain hundreds of thousands of concepts. Especially due to the large size of ontologies, visualisation is useful for authoring, exploring and understanding their underlying data. Existing ontology visualisation tools generally focus on the hierarchical structure, giving much less emphasis to non-hierarchical associations. In this paper we present OntoPlot, a novel visualisation specifically designed to facilitate the exploration of all concept associations whilst still showing an ontology's large hierarchical structure. This hybrid visualisation combines icicle plots, visual compression techniques and interactivity, improving space-efficiency and reducing visual structural complexity. We conducted a user study with domain experts to evaluate the usability of OntoPlot, comparing it with the de facto ontology editor Protege. The results confirm that OntoPlot attains our design goals for association-related tasks and is strongly favoured by domain experts."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934537,JOUR,P5: Portable Progressive Parallel Processing Pipelines for Interactive Data Analysis and Visualization,IEEE Transactions on Visualization and Computer Graphics,1151,1160,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934537,"['Li, J. K.', 'Ma, K.']","[""Data visualization"", ""Graphics processing units"", ""Parallel processing"", ""Data analysis"", ""Grammar"", ""Libraries"", ""Big Data"", ""Information visualization"", ""progressive analytics"", ""visualization software"", ""GPU computing"", ""data exploration""]","We present P5, a web-based visualization toolkit that combines declarative visualization grammar and GPU computing for progressive data analysis and visualization. To interactively analyze and explore big data, progressive analytics and visualization methods have recently emerged. Progressive visualizations of incrementally refining results have the advantages of allowing users to steer the analysis process and make early decisions. P5 leverages declarative grammar for specifying visualization designs and exploits GPU computing to accelerate progressive data processing and rendering. The declarative specifications can be modified during progressive processing to create different visualizations for analyzing the intermediate results. To enable user interactions for progressive data analysis, P5 utilizes the GPU to automatically aggregate and index data based on declarative interaction specifications to facilitate effective interactive visualization. We demonstrate the effectiveness and usefulness of P5 through a variety of example applications and several performance benchmark tests."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934800,JOUR,RSATree: Distribution-Aware Data Representation of Large-Scale Tabular Datasets for Flexible Visual Query,IEEE Transactions on Visualization and Computer Graphics,1161,1171,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934800,"['Mei, H.', 'Chen, W.', 'Wei, Y.', 'Hu, Y.', 'Zhou, S.', 'Lin, B.', 'Zhao, Y.', 'Xia, J.']","[""Visualization"", ""Data visualization"", ""Aggregates"", ""Histograms"", ""Time factors"", ""Visual databases"", ""Social networking (online)"", ""Aggregate query"", ""visual query"", ""large-scale data visualization"", ""R-tree"", ""summed area table"", ""hashing""]","Analysts commonly investigate the data distributions derived from statistical aggregations of data that are represented by charts, such as histograms and binned scatterplots, to visualize and analyze a large-scale dataset. Aggregate queries are implicitly executed through such a process. Datasets are constantly extremely large; thus, the response time should be accelerated by calculating predefined data cubes. However, the queries are limited to the predefined binning schema of preprocessed data cubes. Such limitation hinders analysts' flexible adjustment of visual specifications to investigate the implicit patterns in the data effectively. Particularly, RSATree enables arbitrary queries and flexible binning strategies by leveraging three schemes, namely, an R-tree-based space partitioning scheme to catch the data distribution, a locality-sensitive hashing technique to achieve locality-preserving random access to data items, and a summed area table scheme to support interactive query of aggregated values with a linear computational complexity. This study presents and implements a web-based visual query system that supports visual specification, query, and exploration of large-scale tabular data with user-adjustable granularities. We demonstrate the efficiency and utility of our approach by performing various experiments on real-world datasets and analyzing time and space complexity."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934307,JOUR,GPGPU Linear Complexity t-SNE Optimization,IEEE Transactions on Visualization and Computer Graphics,1172,1181,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934307,"['Pezzotti, N.', 'Thijssen, J.', 'Mordvintsev, A.', 'Hollt, T.', 'Lew, B. Van', 'Lelieveldt, B. P. F.', 'Eisemann, E.', 'Vilanova, A.']","[""Minimization"", ""Linear programming"", ""Computational modeling"", ""Approximation algorithms"", ""Complexity theory"", ""Optimization"", ""Data visualization"", ""High Dimensional Data"", ""Dimensionality Reduction"", ""Progressive Visual Analytics"", ""Approximate Computation"", ""GPGPU""]","In recent years the t-distributed Stochastic Neighbor Embedding (t-SNE) algorithm has become one of the most used and insightful techniques for exploratory data analysis of high-dimensional data. It reveals clusters of high-dimensional data points at different scales while only requiring minimal tuning of its parameters. However, the computational complexity of the algorithm limits its application to relatively small datasets. To address this problem, several evolutions of t-SNE have been developed in recent years, mainly focusing on the scalability of the similarity computations between data points. However, these contributions are insufficient to achieve interactive rates when visualizing the evolution of the t-SNE embedding for large datasets. In this work, we present a novel approach to the minimization of the t-SNE objective function that heavily relies on graphics hardware and has linear computational complexity. Our technique decreases the computational cost of running t-SNE on datasets by orders of magnitude and retains or improves on the accuracy of past approximated techniques. We propose to approximate the repulsive forces between data points by splatting kernel textures for each data point. This approximation allows us to reformulate the t-SNE minimization problem as a series of tensor operations that can be efficiently executed on the graphics card. An efficient implementation of our technique is integrated and available for use in the widely used Google TensorFlow.js, and an open-source C++ library."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934667,JOUR,Galex: Exploring the Evolution and Intersection of Disciplines,IEEE Transactions on Visualization and Computer Graphics,1182,1192,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934667,"['Li, Z.', 'Zhang, C.', 'Jia, S.', 'Zhang, J.']","[""Data visualization"", ""Visualization"", ""Computer science"", ""Semantics"", ""Brushes"", ""Skeleton"", ""Data mining"", ""Science evolution"", ""science mapping"", ""interdisciplinary"", ""knowledge domain visualization"", ""visual analysis""]","Revealing the evolution of science and the intersections among its sub-fields is extremely important to understand the characteristics of disciplines, discover new topics, and predict the future. The current work focuses on either building the skeleton of science, lacking interaction, detailed exploration and interpretation or on the lower topic level, missing high-level macro-perspective. To fill this gap, we design and implement Galaxy Evolution Explorer (Galex), a hierarchical visual analysis system, in combination with advanced text mining technologies, that could help analysts to comprehend the evolution and intersection of one discipline rapidly. We divide Galex into three progressively fine-grained levels: discipline, area, and institution levels. The combination of interactions enables analysts to explore an arbitrary piece of history and an arbitrary part of the knowledge space of one discipline. Using a flexible spotlight component, analysts could freely select and quickly understand an exploration region. A tree metaphor allows analysts to perceive the expansion, decline, and intersection of topics intuitively. A synchronous spotlight interaction aids in comparing research contents among institutions easily. Three cases demonstrate the effectiveness of our system."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934603,JOUR,MetricsVis: A Visual Analytics System for Evaluating Employee Performance in Public Safety Agencies,IEEE Transactions on Visualization and Computer Graphics,1193,1203,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934603,"['Zhao, J.', 'Karimzadeh, M.', 'Snyder, L. S.', 'Surakitbanharn, C.', 'Qian, Z. C.', 'Ebert, D. S.']","[""Organizations"", ""Task analysis"", ""Data visualization"", ""Performance evaluation"", ""Visual analytics"", ""Organizational performance analysis"", ""multi-dimensional data"", ""hierarchical relationships"", ""visual analytics""]","Evaluating employee performance in organizations with varying workloads and tasks is challenging. Specifically, it is important to understand how quantitative measurements of employee achievements relate to supervisor expectations, what the main drivers of good performance are, and how to combine these complex and flexible performance evaluation metrics into an accurate portrayal of organizational performance in order to identify shortcomings and improve overall productivity. To facilitate this process, we summarize common organizational performance analyses into four visual exploration task categories. Additionally, we develop MetricsVis, a visual analytics system composed of multiple coordinated views to support the dynamic evaluation and comparison of individual, team, and organizational performance in public safety organizations. MetricsVis provides four primary visual components to expedite performance evaluation: (1) a priority adjustment view to support direct manipulation on evaluation metrics; (2) a reorderable performance matrix to demonstrate the details of individual employees; (3) a group performance view that highlights aggregate performance and individual contributions for each group; and (4) a projection view illustrating employees with similar specialties to facilitate shift assignments and training. We demonstrate the usability of our framework with two case studies from medium-sized law enforcement agencies and highlight its broader applicability to other domains."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934263,JOUR,R-Map: A Map Metaphor for Visualizing Information Reposting Process in Social Media,IEEE Transactions on Visualization and Computer Graphics,1204,1214,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934263,"['Chen, S.', 'Li, S.', 'Chen, S.', 'Yuan, X.']","[""Semantics"", ""Visualization"", ""Diffusion processes"", ""Data visualization"", ""Twitter"", ""Lakes"", ""Social Media"", ""Information Diffusion"", ""Map-like Visual Metaphor""]","We propose R-Map (Reposting Map), a visual analytical approach with a map metaphor to support interactive exploration and analysis of the information reposting process in social media. A single original social media post can cause large cascades of repostings (i.e., retweets) on online networks, involving thousands, even millions of people with different opinions. Such reposting behaviors form the reposting tree, in which a node represents a message and a link represents the reposting relation. In R-Map, the reposting tree structure can be spatialized with highlighted key players and tiled nodes. The important reposting behaviors, the following relations and the semantics relations are represented as rivers, routes and bridges, respectively, in a virtual geographical space. R-Map supports a scalable overview of a large number of information repostings with semantics. Additional interactions on the map are provided to support the investigation of temporal patterns and user behaviors in the information diffusion process. We evaluate the usability and effectiveness of our system with two use cases and a formal user study."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934284,JOUR,Color Crafting: Automating the Construction of Designer Quality Color Ramps,IEEE Transactions on Visualization and Computer Graphics,1215,1225,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934284,"['Smart, S.', 'Wu, K.', 'Szafir, D. A.']","[""Image color analysis"", ""Data visualization"", ""Encoding"", ""Tools"", ""Guidelines"", ""Interpolation"", ""Visualization"", ""Visualization"", ""Aesthetics in Visualization"", ""Color Perception"", ""Visual Design"", ""Design Mining""]","Visualizations often encode numeric data using sequential and diverging color ramps. Effective ramps use colors that are sufficiently discriminable, align well with the data, and are aesthetically pleasing. Designers rely on years of experience to create high-quality color ramps. However, it is challenging for novice visualization developers that lack this experience to craft effective ramps as most guidelines for constructing ramps are loosely defined qualitative heuristics that are often difficult to apply. Our goal is to enable visualization developers to readily create effective color encodings using a single seed color. We do this using an algorithmic approach that models designer practices by analyzing patterns in the structure of designer-crafted color ramps. We construct these models from a corpus of 222 expert-designed color ramps, and use the results to automatically generate ramps that mimic designer practices. We evaluate our approach through an empirical study comparing the outputs of our approach with designer-crafted color ramps. Our models produce ramps that support accurate and aesthetically pleasing visualizations at least as well as designer ramps and that outperform conventional mathematical approaches."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934536,JOUR,Estimating Color-Concept Associations from Image Statistics,IEEE Transactions on Visualization and Computer Graphics,1226,1235,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934536,"['Rathore, R.', 'Leggon, Z.', 'Lessard, L.', 'Schloss, K. B.']","[""Image color analysis"", ""Data visualization"", ""Databases"", ""Google"", ""Semantics"", ""Cognition"", ""Visualization"", ""Visual Reasoning"", ""Visual Communication"", ""Visual Encoding"", ""Color Perception"", ""Color Cognition"", ""Color Categories""]","To interpret the meanings of colors in visualizations of categorical information, people must determine how distinct colors correspond to different concepts. This process is easier when assignments between colors and concepts in visualizations match people's expectations, making color palettes semantically interpretable. Efforts have been underway to optimize color palette design for semantic interpretablity, but this requires having good estimates of human color-concept associations. Obtaining these data from humans is costly, which motivates the need for automated methods. We developed and evaluated a new method for automatically estimating color-concept associations in a way that strongly correlates with human ratings. Building on prior studies using Google Images, our approach operates directly on Google Image search results without the need for humans in the loop. Specifically, we evaluated several methods for extracting raw pixel content of the images in order to best estimate color-concept associations obtained from human ratings. The most effective method extracted colors using a combination of cylindrical sectors and color categories in color space. We demonstrate that our approach can accurately estimate average human color-concept associations for different fruits using only a small set of images. The approach also generalizes moderately well to more complicated recycling-related concepts of objects that can appear in any color."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934431,JOUR,Searching the Visual Style and Structure of D3 Visualizations,IEEE Transactions on Visualization and Computer Graphics,1236,1245,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934431,"['Hoque, E.', 'Agrawala, M.']","[""Data visualization"", ""Visualization"", ""Encoding"", ""Search engines"", ""Data mining"", ""Indexes"", ""Image color analysis"", ""visualization search engine"", ""visualization design"", ""search user interfaces""]","We present a search engine for D3 visualizations that allows queries based on their visual style and underlying structure. To build the engine we crawl a collection of 7860 D3 visualizations from the Web and deconstruct each one to recover its data, its data-encoding marks and the encodings describing how the data is mapped to visual attributes of the marks. We also extract axes and other non-data-encoding attributes of marks (e.g., typeface, background color). Our search engine indexes this style and structure information as well as metadata about the webpage containing the chart. We show how visualization developers can search the collection to find visualizations that exhibit specific design characteristics and thereby explore the space of possible designs. We also demonstrate how researchers can use the search engine to identify commonly used visual design patterns and we perform such a demographic design analysis across our collection of D3 charts. A user study reveals that visualization developers found our style and structure based search engine to be significantly more useful and satisfying for finding different designs of D3 charts, than a baseline search engine that only allows keyword search over the webpage containing a chart."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934556,JOUR,The Role of Latency and Task Complexity in Predicting Visual Search Behavior,IEEE Transactions on Visualization and Computer Graphics,1246,1255,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934556,"['Battle, L.', 'Crouser, R. J.', 'Nakeshimana, A.', 'Montoly, A.', 'Chang, R.', 'Stonebraker, M.']","[""Task analysis"", ""Complexity theory"", ""Human computer interaction"", ""Visualization"", ""Data visualization"", ""Delays"", ""Visual search"", ""latency"", ""system response time"", ""SRT""]","Latency in a visualization system is widely believed to affect user behavior in measurable ways, such as requiring the user to wait for the visualization system to respond, leading to interruption of the analytic flow. While this effect is frequently observed and widely accepted, precisely how latency affects different analysis scenarios is less well understood. In this paper, we examine the role of latency in the context of visual search, an essential task in data foraging and exploration using visualization. We conduct a series of studies on Amazon Mechanical Turk and find that under certain conditions, latency is a statistically significant predictor of visual search behavior, which is consistent with previous studies. However, our results also suggest that task type, task complexity, and other factors can modulate the effect of latency, in some cases rendering latency statistically insignificant in predicting user behavior. This suggests a more nuanced view of the role of latency than previously reported. Building on these results and the findings of prior studies, we propose design guidelines for measuring and interpreting the effects of latency when evaluating performance on visual search tasks."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934671,JOUR,A Natural-language-based Visual Query Approach of Uncertain Human Trajectories,IEEE Transactions on Visualization and Computer Graphics,1256,1266,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934671,"['Huang, Z.', 'Zhao, Y.', 'Chen, W.', 'Gao, S.', 'Yu, K.', 'Xu, W.', 'Tang, M.', 'Zhu, M.', 'Xu, M.']","[""Trajectory"", ""Semantics"", ""Visualization"", ""Natural languages"", ""Indexing"", ""Spatiotemporal phenomena"", ""Engines"", ""Natural-language-based Visual Query"", ""Spatial Uncertaity"", ""Trajectory Exploration""]","Visual querying is essential for interactively exploring massive trajectory data. However, the data uncertainty imposes profound challenges to fulfill advanced analytics requirements. On the one hand, many underlying data does not contain accurate geographic coordinates, e.g., positions of a mobile phone only refer to the regions (i.e., mobile cell stations) in which it resides, instead of accurate GPS coordinates. On the other hand, domain experts and general users prefer a natural way, such as using a natural language sentence, to access and analyze massive movement data. In this paper, we propose a visual analytics approach that can extract spatial-temporal constraints from a textual sentence and support an effective query method over uncertain mobile trajectory data. It is built up on encoding massive, spatially uncertain trajectories by the semantic information of the POls and regions covered by them, and then storing the trajectory documents in text database with an effective indexing scheme. The visual interface facilitates query condition specification, situation-aware visualization, and semantic exploration of large trajectory data. Usage scenarios on real-world human mobility datasets demonstrate the effectiveness of our approach."
TVCG-2020-1.ris,TVCG,Vis,10.1109/TVCG.2019.2934666,JOUR,You can't always sketch what you want: Understanding Sensemaking in Visual Query Systems,IEEE Transactions on Visualization and Computer Graphics,1267,1277,2020,IEEE Transactions on Visualization and Computer Graphics,1,['2160-9306'],26,26,IEEE Transactions on Visualization and Computer Graphics,Jan. 2020,,https://doi.org/10.1109/TVCG.2019.2934666,"['Lee, D. J.', 'Lee, J.', 'Siddiqui, T.', 'Kim, J.', 'Karahalios, K.', 'Parameswaran, A.']","[""Visualization"", ""Data visualization"", ""Interviews"", ""User centered design"", ""Taxonomy"", ""Buildings"", ""Pattern matching"", ""Visual analytics"", ""exploratory analysis"", ""visual queries""]","Visual query systems (VQSs) empower users to interactively search for line charts with desired visual patterns, typically specified using intuitive sketch-based interfaces. Despite decades of past work on VQSs, these efforts have not translated to adoption in practice, possibly because VQSs are largely evaluated in unrealistic lab-based settings. To remedy this gap in adoption, we collaborated with experts from three diverse domains--astronomy, genetics, and material science--via a year-long user-centered design process to develop a VQS that supports their workflow and analytical needs, and evaluate how VQSs can be used in practice. Our study results reveal that ad-hoc sketch-only querying is not as commonly used as prior work suggests, since analysts are often unable to precisely express their patterns of interest. In addition, we characterize three essential sensemaking processes supported by our enhanced VQS. We discover that participants employ all three processes, but in different proportions, depending on the analytical needs in each domain. Our findings suggest that all three sensemaking processes must be integrated in order to make future VQSs useful for a wide range of analytical inquiries."
